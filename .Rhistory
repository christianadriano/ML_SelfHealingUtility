outcomes <- matrix(1:5, ncol = 5, nrow = 1)
colnames(outcomes)<-c("precision","recall","sensibility","sensitivity","accuracy");
outcomes
outcomes$precision <- 1
outcomes
outcomes<- matrix();
outcomes
colnames(outcomes)<-c("precision","recall","sensibility","sensitivity","accuracy");
colnames(outcomes)<-c("precision");
outcomes
outcomes$precision<-1
outcomes
outcomes<- data.frame(1;2;3;4;5);
outcomes<- list(precision=0, recall=0, sensibility=0, sensitivity=0, accuracy=0);
outcomesMatrix <- rbind(outcomesMatrix,outcomes);
outcomesf<-data.frame(outcomes);
accumOutcomes <- rbind(accumOutcomes,outcomesf);
accumOutcomes <- rbind(accumOutcomes,outcomesf);
accumOutcomes<- data.frame();
accumOutcomes <- rbind(outcomesf,outcomesf);
accumOutcomes<- list();
accumOutcomes <- rbind(accumOutcomes,outcomesf);
accumOutcomes
outcomes<- list(precision=1, recall=0, sensibility=0, sensitivity=0, accuracy=0);
outcomesf<-data.frame(outcomes);
accumOutcomes <- rbind(accumOutcomes,outcomesf);
accumOutcomes
data<- c(2:2.5,0.1)
data
data<- c(2:2.5)
data
?c
c(2:5)
start <- 2
end <- 2.5
data <- seq(a,b,sign(b-a)*0.5)
data <- seq(start,end,sign(b-a)*0.5)
data <- seq(start,end,sign(end-start)*0.5)
data
data <- seq(start,end,sign(end-start)*0.1)
data
logY <- log(data)
logY
x <- c(1:6)
x
plot(x=x,y=data)
plot(x=x,y=logY)
invLogY=1/logY
invData <- 1/data
plot(x=x,y=invData)
plot(x=x,y=invLogY)
plot(x=x,y=invData)
logY <- log2(data)
invLogY=1/logY
plot(x=x,y=invLogY)
featuresdf <- scrambleData(featuresdf)
#Scramble the dataset before extracting the training set.
scrambleData<-function(dataf){
set.seed(8850);
g<- runif((nrow(dataf))); #generates a random distribution
dataf <- dataf[order(g),];
return (dataf);
}
featuresdf <- scrambleData(featuresdf)
featuresdf <- scrambleData(featuresdf)
featuresdf <- scrambleData(dataf=featuresdf)
nrow(featuresdf)
library(xgboost)
library(pROC)
# load data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<-loadData(fileName="data//Random_10000Failures_without_inapplicable_rules.csv");
#dataf<-dataf[dataf$UTILITY.INCREASE!=0,] #Not removing anymore. We need some negative instances
#summary(dataf);
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE, dataf$ADT, dataf$UTILITY.INCREASE);
colnames(featuresdf) <- c("Connectivity", "Criticality","Reliability","Importance","Provided.Interface",
"Required.Interface","ADT","Utility.Increase");
featuresdf <- scrambleData(dataf=featuresdf)
proportion <- 0.7
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration = xgboost.cv$best_iteration
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration = xgboost.cv$best_iteration
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration = xgboost.cv$best_iteration
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration = xgboost.cv$best_iteration
best_iteration
xgboost.cv
xgboost.cv[204]
xgboost.cv$best_iteration
xgboost.cv$evaluation_log[204]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.model
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.model
xgb.test.data = xgb.DMatrix(data.matrix(validationData[,1:7]), missing = NA)
xgb.preds = predict(xgb.model, xgb.test.data)
xgb.roc_obj <- roc(validationData[,"Utility.Increase"], xgb.preds)
xgb.roc_obj
library(xgboost)
library(pROC)
# load data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<-loadData(fileName="data//Random_10000Failures_without_inapplicable_rules.csv");
#dataf<-dataf[dataf$UTILITY.INCREASE!=0,] #Not removing anymore. We need some negative instances
#summary(dataf);
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE, dataf$ADT, dataf$UTILITY.INCREASE);
colnames(featuresdf) <- c("Connectivity", "Criticality","Reliability","Importance","Provided.Interface",
"Required.Interface","ADT","Utility.Increase");
##TODO
#Create a Montecarlo Simulation loop to average the outcome of R2 and RMSE for different samplings
#of the same training/testing/validation split
#Create a loop to compute the outcomes for differnt splits (measure the time of execution)
#Run the simulation for different file sizes 100, 1000, 10000
##Nice to do - see if you can use vectorization to implment these loops (so you can even a blog post about it)
#PLOT function for each feature. Do that using a scatter plot with bloxplot
#Compute the RMSE and R2 for the case of probabilistic output (ask a question on Reddit or StackExchange?)
#Why R2 is not good to select models? How to use AUC, PROC, AIC, BIC instead?
proportion <- 0.7
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
xgboost.cv$evaluation_log[xgboost.cv$best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.model
xgb.test.data = xgb.DMatrix(data.matrix(validationData[,1:7]), missing = NA)
xgb.preds = predict(xgb.model, xgb.test.data)
xgb.roc_obj <- roc(validationData[,"Utility.Increase"], xgb.preds)
xgb.roc_obj
proportion <- 0.9
featuresdf <- scrambleData(dataf=featuresdf);
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
# Compute AUC -------------------------------------------------------------
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.model
#Best training iteration = 17
# iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std
# 17        110.4483        8.86739       225.6595      77.05662
xgb.test.data = xgb.DMatrix(data.matrix(validationData[,1:7]), missing = NA)
xgb.preds = predict(xgb.model, xgb.test.data)
xgb.roc_obj <- roc(validationData[,"Utility.Increase"], xgb.preds)
xgb.roc_obj
?roc
xgb.roc_obj <- multiclass.roc(validationData[,"Utility.Increase"], xgb.preds, )
xgb.roc_obj <- multiclass.roc(validationData[,"Utility.Increase"], xgb.preds)
library(xgboost)
library(pROC)
# load data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<-loadData(fileName="data//Random_10000Failures_without_inapplicable_rules.csv");
#dataf<-dataf[dataf$UTILITY.INCREASE!=0,] #Not removing anymore. We need some negative instances
#summary(dataf);
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE, dataf$ADT, dataf$UTILITY.INCREASE);
colnames(featuresdf) <- c("Connectivity", "Criticality","Reliability","Importance","Provided.Interface",
"Required.Interface","ADT","Utility.Increase");
##TODO
#Create a Montecarlo Simulation loop to average the outcome of R2 and RMSE for different samplings
#of the same training/testing/validation split
#Create a loop to compute the outcomes for differnt splits (measure the time of execution)
#Run the simulation for different file sizes 100, 1000, 10000
##Nice to do - see if you can use vectorization to implment these loops (so you can even a blog post about it)
#PLOT function for each feature. Do that using a scatter plot with bloxplot
#Compute the RMSE and R2 for the case of probabilistic output (ask a question on Reddit or StackExchange?)
#Why R2 is not good to select models? How to use AUC, PROC, AIC, BIC instead?
proportion <- 0.9
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='auc')
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.model
xgb.test.data = xgb.DMatrix(data.matrix(validationData[,1:7]), missing = NA)
xgb.preds = predict(xgb.model, xgb.test.data)
xgb.roc_obj <- multiclass.roc(validationData[,"Utility.Increase"], xgb.preds)
xgb.roc_obj <- roc(validationData[,"Utility.Increase"], xgb.preds)
xgb.roc_obj
proportion <- 0.8
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
# Compute AUC -------------------------------------------------------------
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.model
xgb.test.data = xgb.DMatrix(data.matrix(validationData[,1:7]), missing = NA)
xgb.preds = predict(xgb.model, xgb.test.data)
xgb.roc_obj <- roc(validationData[,"Utility.Increase"], xgb.preds)
xgb.roc_obj
rmse <- function(error){
sqrt(mean(error^2))
}
# Coefficient of determination
# https://en.wikipedia.org/wiki/Coefficient_of_determination
r_squared <- function(prediction, actual){
SS_ExplainedVariance <- sum((prediction - actual)^2);
SS_TotalVariance <- sum((actual-mean(actual))^2);
R2<- 1- SS_ExplainedVariance / SS_TotalVariance;
return (R2);
}
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility.Increase;
refError <- error/validationData$Utility.Increase
plot(refError) + title(main="Relative error")
meanError <- mean(y_pred - validationData$Utility.Increase)
percentMeanError <- meanError / mean(validationData$Utility.Increase)*100;
percentMeanError
rmse_value <- rmse(error);
rmse_value
prediction <- data.frame(y_pred);
actual <- data.frame(validationData$Utility.Increase);
R2 <- r_squared(prediction,as.numeric(validationData$Utility.Increase));
R2
plot(error) + title(main="Training error")
xyplot(y_pred ~Utility.Increase,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
library(lattice)
xyplot(y_pred ~Utility.Increase,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
xyplot(y_pred ~ Require.Interface,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
xyplot(y_pred ~ Required.Interface,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
xyplot(y_pred ~ Provided.Interface,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
cor(y_pred,validationData$Utility.Increase)
cor(y_pred,validationData$Provided.Interface)
model <- lm(y_pred ~ Provided.Interface, validationData)
summary(model)
xyplot(y_pred ~ Connectivity,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
xyplot(y_pred ~ Criticality,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
cor(validationData$Provided.Interface,validationData$Criticality)
xyplot(Provided.Interface ~ Criticality,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
# Investigate correlations among explanatory variables
install.packages("Hmisc")
library("Hmisc")
# load data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<-loadData(fileName="data//Random_10000Failures_without_inapplicable_rules.csv");
matrixInput<-data.frame(dataf$UTILITY.DROP,dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY);
colnames(matrixInput)<-c("Utility_Drop","Criticality","Connectivity","Reliability");
matrixInput<-data.frame(dataf$UTILITY.DROP,dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY);
res <- rcorr(data.matrix(matrixInput),type=c("spearman"));
res$r
dataf<-loadData(fileName="data//Random_10000Failures_without_inapplicable_rules.csv");
matrixInput<-data.frame(dataf$UTILITY.DROP,dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY);
matrixInput<-data.frame(dataf$UTILITY.INCREASE,dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,
dataf$IMPORTANCE, dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,dataf$ADT);
colnames(matrixInput)<-c("Utility.Increase","Criticality","Connectivity","Reliability","Importance",
"Provided.Inteface","Required.Interface","ADT");
res <- rcorr(data.matrix(matrixInput),type=c("spearman"));
res$r
cor(dataf$Provided.Interface,dataf$Criticality)
cor(dataf$PROVIDED_INTERFACE,dataf$CRITICALITY)
plot(dataf$CRITICALITY,dataf$PROVIDED_INTERFACE)
plot(validationData$Provided.Interface,validationData$Criticality)
plot(trainingData$Provided.Interface,trainingData$Criticality)
library(xgboost)
library(pROC)
# load data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<-loadData(fileName="data//Random_10000Failures_without_inapplicable_rules.csv");
#dataf<-dataf[dataf$UTILITY.INCREASE!=0,] #Not removing anymore. We need some negative instances
#summary(dataf);
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE, dataf$ADT, dataf$UTILITY.INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance","Provided.Interface",
"Required.Interface","ADT","Utility.Increase");
##TODO
#COMPUTE CORRELATIONS AMONG FEATURES!!!
#Create a Montecarlo Simulation loop to average the outcome of R2 and RMSE for different samplings
#of the same training/testing/validation split
#Create a loop to compute the outcomes for differnt splits (measure the time of execution)
#Run the simulation for different file sizes 100, 1000, 10000
##Nice to do - see if you can use vectorization to implment these loops (so you can even a blog post about it)
#PLOT function for each feature. Do that using a scatter plot with bloxplot
#Compute the RMSE and R2 for the case of probabilistic output (ask a question on Reddit or StackExchange?)
#Why R2 is not good to select models? How to use AUC, PROC, AIC, BIC instead?
proportion <- 0.8
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.model
xgb.test.data = xgb.DMatrix(data.matrix(validationData[,1:7]), missing = NA)
xgb.preds = predict(xgb.model, xgb.test.data)
xgb.roc_obj <- roc(validationData[,"Utility.Increase"], xgb.preds)
xgb.roc_obj
# Call:
rmse <- function(error){
sqrt(mean(error^2))
}
# Coefficient of determination
# https://en.wikipedia.org/wiki/Coefficient_of_determination
r_squared <- function(prediction, actual){
SS_ExplainedVariance <- sum((prediction - actual)^2);
SS_TotalVariance <- sum((actual-mean(actual))^2);
R2<- 1- SS_ExplainedVariance / SS_TotalVariance;
return (R2);
}
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility.Increase;
refError <- error/validationData$Utility.Increase
plot(refError) + title(main="Relative error")
meanError <- mean(y_pred - validationData$Utility.Increase)
percentMeanError <- meanError / mean(validationData$Utility.Increase)*100;
percentMeanError
rmse_value <- rmse(error);
rmse_value
prediction <- data.frame(y_pred);
actual <- data.frame(validationData$Utility.Increase);
R2 <- r_squared(prediction,as.numeric(validationData$Utility.Increase));
R2
plot(error) + title(main="Training error")
library(lattice)
xyplot(y_pred ~ Criticality,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
#https://www.stat.ubc.ca/~jenny/STAT545A/block09_xyplotLattice.html
cor(y_pred,validationData$Provided.Interface)
xyplot(Provided.Interface ~ Criticality,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
xyplot(y_pred ~ Provided.Interface,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
xyplot(y_pred ~ ADT,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
xyplot(y_pred ~ Connectivity,validationData, grid=TRUE,
type = c("p", "smooth"), col.line = "darkorange", lwd = 1, main="Predicted versus Actual");
cor(validationData$Provided.Interface,validationData$Criticality)
plot(validationData$Provided.Interface,validationData$Criticality)
plot(trainingData$Provided.Interface,trainingData$Criticality)
plot(trainingData$Provided.Interface,trainingData$Connectivity)
matrixInput<-data.frame(dataf$UTILITY.INCREASE,dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,
dataf$IMPORTANCE, dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,dataf$ADT);
colnames(matrixInput)<-c("Utility.Increase","Criticality","Connectivity","Reliability","Importance",
"Provided.Inteface","Required.Interface","ADT");
matrixInput<-data.frame(dataf$UTILITY.INCREASE,dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,
dataf$IMPORTANCE, dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,dataf$ADT);
colnames(matrixInput)<-c("Utility.Increase","Criticality","Connectivity","Reliability","Importance",
"Provided.Inteface","Required.Interface","ADT");
res$r
cor(validationData$Provided.Interface,validationData$Connectivity)
cor(trainingData$Provided.Interface,trainingData$Connectivity)
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY)
res <- rcorr(data.matrix(matrixInput));type=c("spearman"));
res <- rcorr(data.matrix(matrixInput));#type=c("spearman"));
res$r
shapiro.test(dataf$CONNECTIVITY);
shapiro.test(dataf$RELIABILITY);
dim(dataf)
qqplot(dataf$CONNECTIVITY)
qqnorm(dataf$CRITICALITY,main="Normal Q-Q Plot - Criticality")+ qqline();
qqline(dataf$CRITICALITY)
qqnorm(dataf$CONNECTIVITY,main="Normal Q-Q Plot - Connectivity") + qqline();
qqline(dataf$CONNECTIVITY)
qqnorm(dataf$RELIABILITY,main="Normal Q-Q Plot - Reliability")
qqline(dataf$RELIABILITY)
qqnorm(dataf$PROVIDED_INTERFACE,main="Normal Q-Q Plot - Provided.Interface")
qqline(dataf$PROVIDED_INTERFACE)
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY,type=c("spearman"))
?cor
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY,method=c("kendall"))
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY,method=c("spearman"))
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY,method=c("pearson"))
plot(dataf$CRITICALITY,dataf$PROVIDED_INTERFACE)
plot(dataf$CONNECTIVITY,dataf$PROVIDED_INTERFACE)
hist(dataf$PROVIDED_INTERFACE)
plot(dataf$CONNECTIVITY,log(dataf$PROVIDED_INTERFACE+1))
hist(log(dataf$PROVIDED_INTERFACE+1))
qqnorm(log(dataf$PROVIDED_INTERFACE+1),main="Normal Q-Q Plot - Provided.Interface")
qqline(log(dataf$PROVIDED_INTERFACE+1))
res <- rcorr(data.matrix(matrixInput));#type=c("spearman"));
res$r
res <- rcorr(data.matrix(matrixInput),type=c("spearman"));
res <- rcorr(data.matrix(matrixInput),type=c("spearman"));
res$r
res$P
plot(dataf$CONNECTIVITY,dataf$PROVIDED_INTERFACE)
hist(log(dataf$PROVIDED_INTERFACE+1))
hist(dataf$PROVIDED_INTERFACE)
dataf <- dataf[dataf$PROVIDED_INTERFACE!=0,]
res <- rcorr(data.matrix(matrixInput),type=c("spearman"));
res$r
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY,method=c("pearson"))
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY,method=c("kendall"))
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY,method=c("spearman"))
plot(dataf$CONNECTIVITY,dataf$PROVIDED_INTERFACE)
hist(dataf$PROVIDED_INTERFACE)
plot(dataf$CONNECTIVITY,dataf$PROVIDED_INTERFACE)
res <- rcorr(data.matrix(matrixInput),type=c("spearman"));
res$r
dataf<-loadData(fileName="data//Random_10000Failures_without_inapplicable_rules.csv");
matrixInput<-data.frame(dataf$UTILITY.INCREASE,dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,
dataf$IMPORTANCE, dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,dataf$ADT);
colnames(matrixInput)<-c("Utility.Increase","Criticality","Connectivity","Reliability","Importance",
"Provided.Inteface","Required.Interface","ADT");
res <- rcorr(data.matrix(matrixInput),type=c("spearman"));
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY,method=c("pearson"))
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY,method=c("kendall"))
cor(dataf$PROVIDED_INTERFACE,dataf$CONNECTIVITY,method=c("spearman"))
plot(dataf$CONNECTIVITY,dataf$PROVIDED_INTERFACE)
hist(dataf$PROVIDED_INTERFACE)
dataf <- dataf[dataf$PROVIDED_INTERFACE!=0,]
hist(dataf$PROVIDED_INTERFACE)
dataf$PROVIDED_INTERFACE!=0
plot(dataf$CONNECTIVITY,dataf$PROVIDED_INTERFACE)
