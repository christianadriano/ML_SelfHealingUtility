"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY)
# dataf$IMPORTANCE,
#                         dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,
#                         dataf$PMax,dataf$alpha,dataf$REQUEST,
#                         dataf$UTILITY_INCREASE);
# # dataf$ADT,
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability")
# ,"Importance",
#                           "Provided_Interface",
#                           "Required_Interface",
#                           "PMax","alpha","REQUEST",
#                           "Utility_Increase");
# #"ADT",
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 1;
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:9]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
library(xgboost)
# load data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf_l<-loadData(fileName="data//Linear.csv");
# dataf_p<-loadData(fileName="data//Probabilistic.csv");
# dataf_d <- loadData(fileName="data//discontinous.csv");
#dataf_s <- loadData(fileName="data//10000//Saturating50000.csv");
#dataf_a <- loadData(fileName="data//10000//ALL44K.csv");
dataf <- dataf_l;
#summary(dataf_s)
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=7));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY)
# dataf$IMPORTANCE,
#                         dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,
#                         dataf$PMax,dataf$alpha,dataf$REQUEST,
#                         dataf$UTILITY_INCREASE);
# # dataf$ADT,
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability")
# ,"Importance",
#                           "Provided_Interface",
#                           "Required_Interface",
#                           "PMax","alpha","REQUEST",
#                           "Utility_Increase");
# #"ADT",
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 1;
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
trainingData[,1:3]
featuresdf <- scrambleData(dataf=featuresdf);
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
trainingData[,1:3]
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability", "Utility_Increase");
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
dataf_l<-loadData(fileName="data//Linear.csv");
dataf <- dataf_l;
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=7));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
featuresf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
featuresf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
summary(dataf)
dataf_l<-loadData(fileName="data//100//Linear100.csv");
dataf <- dataf_l;
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=7));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability", "Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 1;
for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:3]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
}
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 1;
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:3]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
resultsf
dataf_p<-loadData(fileName="data//100//Probabilistic100.csv");
dataf <- dataf_p;
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=7));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability", "Utility_Increase");
dataf_l<-loadData(fileName="data//100//Linear100.csv"); #3.96% MAPD
dataf <- dataf_l;
resultsf <- data.frame(matrix(data=NA,nrow=5,ncol=7));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability", "Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 1;
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:3]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
dataf_p<-loadData(fileName="data//100//Probabilistic100.csv");
dataf <- dataf_p;
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability", "Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 1;
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:3]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
dataf_l<-loadData(fileName="data//100//Linear100.csv"); #3.96% MAPD
dataf <- dataf_l;
resultsf <- data.frame(matrix(data=NA,nrow=5,ncol=7));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability", "Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 1;
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:3]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
dataf_p<-loadData(fileName="data//100//Probabilistic100.csv");
dataf <- dataf_p;
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability", "Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 2;
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:3]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
dataf_d <- loadData(fileName="data//100//discontinous100.csv");
dataf <- dataf_d;
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability", "Utility_Increase");
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,
dataf$PMax,dataf$alpha,dataf$REQUEST,dataf$ADT,
dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance",
"Provided_Interface",
"Required_Interface",
"PMax","alpha","REQUEST","ADT",
"Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 3;
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,
dataf$ADT,
dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance",
"Provided_Interface",
"Required_Interface",
"ADT",
"Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 3;
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
dataf_s <- loadData(fileName="data//10000//Saturating50000.csv");
dataf <- dataf_s;
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,
dataf$PMax,dataf$alpha,dataf$REQUEST,
dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance",
"Provided_Interface",
"Required_Interface",
"PMax","alpha","REQUEST",
"Utility_Increase");
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance",
"Provided_Interface",
"Required_Interface",
"PMax","alpha","REQUEST",
"Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 4;
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:9]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,
dataf$PMax,dataf$alpha,dataf$REQUEST,dataf$ADT,
dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance",
"Provided_Interface",
"Required_Interface",
"PMax","alpha","REQUEST","ADT",
"Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 5;
dataf_a <- loadData(fileName="data//10000//ALL44K.csv");
dataf_a <- loadData(fileName="data//10000//ALL44K.csv");
dataf <- dataf_a;
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,
dataf$PMax,dataf$alpha,dataf$REQUEST,dataf$ADT,
dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance",
"Provided_Interface",
"Required_Interface",
"PMax","alpha","REQUEST","ADT",
"Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 5;
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:9]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
resultsf
