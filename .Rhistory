#Load utility functions
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
library(xgboost)
library(r2pmml) #https://github.com/jpmml/r2pmml
a <- {1.4, 2.5, 3.1, 5.5}
a <- c(1.4, 2.5, 3.1, 5.5)
b <- c(2.1, 3.9, 4.3, 6.7)
sd(a)
sd(b)
1/2*100
100 * 1/2
100 * 2/3
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
#Download and install the latest H2O package for R.
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))
library(h2o)
h2o.init()
demo(h2o.kmeans)
install.packages("gbm")
install.packages("devtools")
install_git("git://github.com/jpmml/r2pmml.git")
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//DataPoints_1K-3K-9K//";
modelList <- c("Linear","Discontinuous","Saturating","ALL");
modelName <- modelList[1];
datasetSize <- c("1K","3K","9K");
datasetName <- generateDataSetNames(modelName,datasetSize,0);
# Generate the dataset names that will be trained ----------------------------
generateDataSetNames <- function(modelName,datasetSize,s_idx){
if(s_idx==0 & length(datasetSize)>0){#Generate for all sizes
datasetName <- paste0(modelName,datasetSize[1]);
for(i in c(2:length(datasetSize))){
datasetName <- cbind(datasetName,paste0(modelName,datasetSize[i]));
}
}
else{
datasetName <- paste0(modelName,datasetSize[s_idx]);
}
return(datasetName);
}
# Save results to file ----------------------------------------------------
resultsToFile <- function(mcResults,modelName,extension){
fileName <- paste0("mcResultsf_",modelName,extension);
write.table(mcResults,fileName,sep=",",col.names = TRUE);
print(paste0("file written:",fileName));
mcResults
}
# Prepare features --------------------------------------------------------
prepareFeatures <- function(dataf,selectionType){
#Do feature selection (or not)
if(selectionType=="ALL")
featuresdf<- select_ALL(dataf)
else
if(selectionType=="Linear")
featuresdf<- select_Linear(dataf)
else
if(selectionType=="Discontinuous")
featuresdf<- select_Discontinuous(dataf)
else
if(selectionType=="Saturating")
featuresdf<- select_Saturation(dataf)
#Remove zero utilities
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
# Scramble data
featuresdf <- scrambleData(datadf=featuresdf);
return (featuresdf);
}
library(devtools)
library(xgboost)
library(r2pmml) #https://github.com/jpmml/r2pmml
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//DataPoints_1K-3K-9K//";
modelList <- c("Linear","Discontinuous","Saturating","ALL");
modelName <- modelList[3];
datasetSize <- c("1K","3K","9K");
datasetName <- generateDataSetNames(modelName,datasetSize,0);
modelName <- modelList[1];
datasetSize <- c("1K","3K","9K");
datasetName <- generateDataSetNames(modelName,datasetSize,0);
# Generate the dataset names that will be trained -------------------------
generateDataSetNames <- function(modelName,datasetSize,s_idx){
if(s_idx==0 & length(datasetSize)>0){#Generate for all sizes
datasetName <- paste0(modelName,datasetSize[1]);
for(i in c(2:length(datasetSize))){
datasetName <- cbind(datasetName,paste0(modelName,datasetSize[i]));
}
}
else{
datasetName <- paste0(modelName,datasetSize[s_idx]);
}
return(datasetName);
}
resultsToFile <- function(mcResults,modelName,extension){
fileName <- paste0("mcResultsf_",modelName,extension);
write.table(mcResults,fileName,sep=",",col.names = TRUE);
print(paste0("file written:",fileName));
mcResults
}
prepareFeatures <- function(dataf,selectionType){
#Do feature selection (or not)
if(selectionType=="ALL")
featuresdf<- select_ALL(dataf)
else
if(selectionType=="Linear")
featuresdf<- select_Linear(dataf)
else
if(selectionType=="Discontinuous")
featuresdf<- select_Discontinuous(dataf)
else
if(selectionType=="Saturating")
featuresdf<- select_Saturation(dataf)
#Remove zero utilities
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
# Scramble data
featuresdf <- scrambleData(datadf=featuresdf);
return (featuresdf);
}
trainModel <- function(featuresdf){
inputFeatures <- dim(featuresdf)[2] - 1; #last column is the target variable
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)# booster="gbtree")
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 2500,
early_stopping_rounds = 500, metrics='rmse',verbose = FALSE)
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
return(list(xgb.model,xgboost.cv));
}
validatePredictions <- function(modelList, mcResultsf,validationData){
xgb.model <- modelList[[1]];
xgboost.cv <- modelList[[2]];
best_iteration <- xgboost.cv$best_iteration;
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$UTILITY_INCREASE;
best_iteration <- xgboost.cv$best_iteration;
mcResultsf$DataSet[i]<-datasetName[i];
mcResultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
mcResultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
mcResultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
mcResultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
mcResultsf$RMSE[i] <- rmse(error);
mcResultsf$R_Squared[i] <- r_squared(y_pred,validationData$UTILITY_INCREASE);
mcResultsf$MAPD[i] <- mapd(y_pred,validationData$UTILITY_INCREASE);
return(mcResultsf);
}
generatePMML <- function(xgb.model, featuresdf,modelName){
inputFeatures <- dim(featuresdf)[2] - 1; #last column is the target variable
# Generate feature map
xgboost.fmap = r2pmml::genFMap(featuresdf[1:inputFeatures])
r2pmml::writeFMap(xgboost.fmap, "xgboost.fmap")
# Save the model in XGBoost proprietary binary format
xgb.save(xgb.model, "xgboost.model")
# Dump the model in text format
#  xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap");
pmmlFileName <- paste0(".//pmml///",modelName,"-xgb.pmml");
r2pmml(xgb.model, pmmlFileName, fmap = xgboost.fmap, response_name = "UTILITY_INCREASE",
missing = NULL, ntreelimit = 25, compact = TRUE)
}
library(devtools)
library(xgboost)
library(r2pmml) #https://github.com/jpmml/r2pmml
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//DataPoints_1K-3K-9K//";
modelList <- c("Linear","Discontinuous","Saturating","ALL");
modelName <- modelList[1];
datasetSize <- c("1K","3K","9K");
datasetName <- generateDataSetNames(modelName,datasetSize,0);
#i <- 2;
fileName <- paste0(folder,datasetName[i],".csv");
i <- 2;
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
featuresdf <- prepareFeatures(dataf,"Saturating");
#Extract training ad validation sets
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
#Train model
outcomeList <- trainModel(trainingData);
trained.model$evaluation_log[best_iteration]
i <- 2;
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
#data_all <- read.csv(fileName,header = TRUE,sep=",");
featuresdf <- prepareFeatures(dataf,"Saturating");
#Extract training ad validation sets
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
#Train model
outcomeList <- trainModel(trainingData);
trainModel <- function(featuresdf){
inputFeatures <- dim(featuresdf)[2] - 1; #last column is the target variable
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)# booster="gbtree")
#Discovers the best model
system.time(trained.model <-  xgb.cv(
param=param,
data = xgb.train.data,
nfold = 10,
nrounds = 2500,
early_stopping_rounds = 500,
metrics='rmse',
verbose = FALSE)
)
best_iteration <- trained.model$best_iteration;
#trained.model$evaluation_log[best_iteration]
#Get the bes model
best.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
return(list(best.model,trained.model));
}
i <- 2;
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
#data_all <- read.csv(fileName,header = TRUE,sep=",");
featuresdf <- prepareFeatures(dataf,"Saturating");
#Extract training ad validation sets
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
#Train model
outcomeList <- trainModel(trainingData);
i <- 2;
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
#data_all <- read.csv(fileName,header = TRUE,sep=",");
featuresdf <- prepareFeatures(dataf,"Saturating");
#Extract training ad validation sets
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
#Train model
outcomeList <- trainModel(trainingData);
outcomeList[3]
outcomeList
outcomeList[0]
outcomeList[1]
outcomeList[2]
outcomeList[3]
outcomeList.length
outcomeList.size
dim(outcomeList)
View(outcomeList)
View(outcomeList)
View(outcomeList)
length(outcomeList)
trainModel <- function(featuresdf){
inputFeatures <- dim(featuresdf)[2] - 1; #last column is the target variable
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)# booster="gbtree")
#Discovers the best model
time <- system.time(trained.model <-  xgb.cv(
param=param,
data = xgb.train.data,
nfold = 10,
nrounds = 2500,
early_stopping_rounds = 500,
metrics='rmse',
verbose = FALSE)
)
best_iteration <- trained.model$best_iteration;
#trained.model$evaluation_log[best_iteration]
#Get the bes model
best.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
return(list(best.model,trained.model,time));
}
i <- 2;
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
#data_all <- read.csv(fileName,header = TRUE,sep=",");
featuresdf <- prepareFeatures(dataf,"Saturating");
#Extract training ad validation sets
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
#Train model
outcomeList <- trainModel(trainingData);
outcomeList[3]
