dataf$PMax,dataf$alpha,dataf$REQUEST,
dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance",
"Provided_Interface",
"Required_Interface",
"PMax","alpha","REQUEST",
"Utility_Increase");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:9]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 1;
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:9]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
resultsf$MAPD
proportion <- 0.9
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 1;
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:9]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
proportionStr <- toString(proportion);
meanRMSE_Train <- toString(round(averageRMSE(resultsf$Train_RMSE_MEAN,trainingSize),2))
title <- paste("Training RMSE, training proportion", proportionStr,"mean=",meanRMSE_Train)
plot(resultsf$Train_RMSE_MEAN, main=title);
meanMAPD_validation <- round(mean(resultsf$MAPD),4);
title <- paste("Validation MAPD, data proportion", proportionStr,"mean=",meanMAPD_validation)
plot(resultsf$MAPD, main=title);
resultsf$MAPD
dataf_s <- loadData(fileName="data//10000//Saturating50000.csv");
dataf <- dataf_s;
#summary(dataf_s)
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=7));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,
dataf$PMax,dataf$alpha,dataf$REQUEST,
dataf$UTILITY_INCREASE);
# dataf$ADT,
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance",
"Provided_Interface",
"Required_Interface",
"PMax","alpha","REQUEST",
"Utility_Increase");
#"ADT",
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$Utility_Increase!=0,];
i <- 1;
for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:9]),
label = trainingData[,"Utility_Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.save(xgb.model,fname="xgd.model.saturating");
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility_Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility_Increase);
resultsf$MAPD[i] <- mapd(y_pred,validationData$Utility_Increase);
}
resultsf$MAPD
proportionStr <- toString(proportion);
meanRMSE_Train <- toString(round(averageRMSE(resultsf$Train_RMSE_MEAN,trainingSize),2))
title <- paste("Training RMSE, training proportion", proportionStr,"mean=",meanRMSE_Train)
plot(resultsf$Train_RMSE_MEAN, main=title);
proportionStr <- toString(1-proportion);
validationSize <- length(validationData$Utility_Increase)
meanRMSE_validation <- toString(round(averageRMSE(resultsf$RMSE,validationSize),2))
title <- paste("Validation RMSE, data proportion", proportionStr,"mean=",meanRMSE_validation)
plot(resultsf$RMSE, main=title);
maxRSquared <- max(resultsf$R_Squared);
minRSquared <- min(resultsf$R_Squared);
title <- paste("Validation R_Squared, data proportion", proportionStr,"max=",maxRSquared,"min=",minRSquared);
plot(resultsf$R_Squared, main=title);
hist(resultsf$R_Squared)
hist(resultsf$RMSE)
meanMAPD_validation <- round(mean(resultsf$MAPD),4);
title <- paste("Validation MAPD, data proportion", proportionStr,"mean=",meanMAPD_validation)
plot(resultsf$MAPD, main=title);
hist(trainingData$Utility_Increase)
hist(resultsf$RMSE)
hist(resultsf$RMSE)
meanRMSE_validation <- toString(round(averageRMSE(resultsf$RMSE,validationSize),2))
title <- paste("Validation RMSE, data proportion", proportionStr,"mean=",meanRMSE_validation)
plot(resultsf$RMSE, main=title);
proportionStr <- toString(proportion);
meanRMSE_Train <- toString(round(averageRMSE(resultsf$Train_RMSE_MEAN,trainingSize),2))
title <- paste("Training RMSE, training proportion", proportionStr,"mean=",meanRMSE_Train)
averageRMSE <- function(RMSEVector, sampleSize){
RMSE_sqr <- (RMSEVector^2) * sampleSize;
RMSE_points <- length(RMSEVector);
return (sum(RMSE_sqr) /(RMSE_points * sampleSize))
}
round(averageRMSE(resultsf$RMSE,validationSize),2)
averageRMSE <- function(RMSEVector, sampleSize){
RMSE_sqr <- (RMSEVector^2) * sampleSize;
RMSE_points <- length(RMSEVector);
return (sum(RMSE_sqr /(RMSE_points * sampleSize)))
}
round(averageRMSE(resultsf$RMSE,validationSize),2)
hist(resultsf$RMSE)
averageRMSE <- function(RMSEVector, sampleSize){
RMSE_sqr <- (RMSEVector^2) * sampleSize*100;
RMSE_points <- length(RMSEVector);
return (sum(RMSE_sqr /(RMSE_points * sampleSize)))
}
round(averageRMSE(resultsf$RMSE,validationSize),2)
error_1 <- 4
error_2 <- 5
error_3 <- 6
rmse(error_1)
errorList <- c(error_1,error_2,error_3)
errorList
rmse(errorList)
errorList2 <- c(1,2,3)
errorList3 <- c(7,8,9)
rmseVector <- c(rmse(errorList),rmse(errorList2),rmse(errorList3))
averageRMSE(RMSEVector=rmseVector,3)
averageRMSE <- function(RMSEVector, sampleSize){
RMSE_sqr <- sqrt((RMSEVector^2) * sampleSize));
RMSE_points <- length(RMSEVector);
return (sum(RMSE_sqr /(RMSE_points * sampleSize)))
}
averageRMSE <- function(RMSEVector, sampleSize){
RMSE_sqr <- sqrt((RMSEVector^2) * sampleSize);
RMSE_points <- length(RMSEVector);
return (sum(RMSE_sqr /(RMSE_points * sampleSize)))
}
averageRMSE(RMSEVector=rmseVector,3)
# load xml and pmml library
library(XML)
library(pmml)
install.packages("pmml")
install.packages("pmml")
install.packages("pmml")
library(pmml)
library("devtools")
install_git("git://github.com/jpmml/r2pmml.git")
library(r2pmml)
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<- loadData(fileName = "data//100//Linear100.csv");
featuresf<-data.frame(dataf$CRITICALITY,
dataf$CONNECTIVITY,
dataf$RELIABILITY,
dataf$UTILITY_INCREASE);
validationf<-data.frame(staticf$CRITICALITY,
staticf$CONNECTIVITY,
staticf$RELIABILITY,
staticf$UTILITY_INCREASE);
featuresf<-data.frame(dataf$CRITICALITY,
dataf$CONNECTIVITY,
dataf$RELIABILITY,
dataf$UTILITY_INCREASE);
colnames(featuresf) <- c("CRITICALITY","CONNECTIVITY","RELIABILITY","UTILITY_INCREASE");
plot(featuresf);
modelFit<- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY)  ,data=featuresf);
modelFit
summary(modelFit)
modelFit<- lm(UTILITY_INCREASE ~ CONNECTIVITY*CRITICALITY + RELIABILITY ,data=featuresf);
summary(modelFit)
modelFit <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY) +log(RELIABILITY),data=featuresf);
summary(modelFit)
CriticalityConnectivity_LM.lm<- lm(UTILITY_INCREASE ~ CRITICALITY*CONNECTIVITY*RELIABILITY ,data=featuresf);
modelFit<- lm(UTILITY_INCREASE ~ CRITICALITY*CONNECTIVITY*RELIABILITY ,data=featuresf);
summary(modelFit)
modelFit<- lm(UTILITY_INCREASE ~ . ,data=featuresf);
summary(modelFit)
modelFit_7 <- lm(UTILITY_INCREASE ~ CRITICALITY*CONNECTIVITY*RELIABILITY ,data=featuresf);
modelFit_1 <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY)  ,data=trainingData);
summary(modelFit_1)
proportion <- 0.8;
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
modelFit_1 <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY)  ,data=trainingData);
dataf<- loadData(fileName = "data//100//Linear100.csv");
dataf<- dataf[dataf$RELIABILITY!=0,];
dataf <- dataf[dataf$UTILITY.INCREASE!=0,];
dataf<- loadData(fileName = "data//100//Linear100.csv");
featuresf<-data.frame(dataf$CRITICALITY,
dataf$CONNECTIVITY,
dataf$RELIABILITY,
dataf$UTILITY_INCREASE);
colnames(featuresf) <- c("CRITICALITY","CONNECTIVITY","RELIABILITY","UTILITY_INCREASE");
plot(featuresf);
title("Training");
proportion <- 0.8;
featuresdf <- scrambleData(dataf=featuresdf);
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
modelFit_1 <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY)  ,data=trainingData);
trainingData
colnames(featuresf) <- c("CRITICALITY","CONNECTIVITY","RELIABILITY","UTILITY_INCREASE");
featuresdf <- scrambleData(dataf=featuresdf);
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<- loadData(fileName = "data//100//Linear100.csv");
#summary(dataf);
featuresdf<-data.frame(dataf$CRITICALITY,
dataf$CONNECTIVITY,
dataf$RELIABILITY,
dataf$UTILITY_INCREASE);
colnames(featuresdf) <- c("CRITICALITY","CONNECTIVITY","RELIABILITY","UTILITY_INCREASE");
plot(featuresdf);
title("Training");
proportion <- 0.8;
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
modelFit_1 <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY)  ,data=trainingData);
summary(modelFit_1);
avPlots(modelFit_1)
library(caret);
library(car);
avPlots(modelFit_1)
modelFit_2 <- lm(UTILITY_INCREASE ~ CONNECTIVITY*CRITICALITY + RELIABILITY ,data=trainingData);
summary(modelFit_2);
lmPredicted <- predict(modelFit_1, validationData)
lmPredicted
summary(lmPredicted)
residual.modelFit<- resid(modelFit_1);
plot(x=featuresf$Criticality,y=residual.modelFit,
ylab="Residuals", xlab="Criticality",
main="Residual Plot Actual Utility minus Predicted Utility");
plot(x=featuresf$Criticality,y=residual.modelFit_1,
ylab="Residuals", xlab="Criticality",
main="Residual Plot Actual Utility minus Predicted Utility");
plot(x=validationData$CRITICALITY,y=residual.modelFit,
ylab="Residuals", xlab="Criticality",
main="Residual Plot Actual Utility minus Predicted Utility");
lmPredicted <- predict(modelFit_1, validationData)
residual.modelFit<- resid(modelFit_1);
plot(x=validationData$CRITICALITY,y=residual.modelFit,
ylab="Residuals", xlab="Criticality",
main="Residual Plot Actual Utility minus Predicted Utility");
lmPredictedValidation <- predict(modelFit_1, validationData);
plot(x=validationData$CONNECTIVITY,y=validationData$UTILITY_INCREASE);
points(validationData$CRITICALITY, lmPredictedValidation, col = "red",
pch=4,abline(modelFit));
title("Linear Regression - actual (circles) vs predicted (crosses)");
error<- validationData$UTILITY_INCREASE - lmPredictedValidation # same as data$Y - predictedY
predictionRMSE <- rmse(error)
predictionRMSE
MADP <- mapd(lmPredictedValidation, validationData$UTILITY_INCREASE)
MAPD
MADP
lmPredictedValidation <- predict(modelFit_2, validationData);
MADP <- mapd(lmPredictedValidation, validationData$UTILITY_INCREASE)
MADP
modelFit_3 <- lm(UTILITY_INCREASE ~ CRITICALITY ,data=trainingData);
modelFit_4<- lm(UTILITY_INCREASE ~ CONNECTIVITY ,data=trainingData);
modelFit_5<- lm(UTILITY_INCREASE ~ RELIABILITY ,data=trainingData);
modelFit_6 <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY) +log(RELIABILITY),data=trainingData);
modelFit_7 <- lm(UTILITY_INCREASE ~ CRITICALITY*CONNECTIVITY*RELIABILITY ,data=trainingData);
modelFit_8 <- lm(UTILITY_INCREASE ~ . ,data=trainingData);
summary(modelFit_2);
summary(modelFit_3);
modelFit_4<- lm(UTILITY_INCREASE ~ CONNECTIVITY ,data=trainingData);
summary(modelFit_4);
modelFit_5<- lm(UTILITY_INCREASE ~ RELIABILITY ,data=trainingData);
summary(modelFit_5);
modelFit_6 <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY) +log(RELIABILITY),data=trainingData);
summary(modelFit_6);
modelFit_7 <- lm(UTILITY_INCREASE ~ CRITICALITY*CONNECTIVITY*RELIABILITY ,data=trainingData);
summary(modelFit_7);
summary(modelFit_1);
lmPredictedValidation <- predict(modelFit_1, validationData);
predictionRMSE <- rmse(error)
predictionRMSE
lmPredictedValidation <- predict(modelFit_2, validationData);
predictionRMSE <- rmse(error)
predictionRMSE
error<- validationData$UTILITY_INCREASE - lmPredictedValidation # same as data$Y - predictedY
predictionRMSE <- rmse(error)
predictionRMSE
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<- loadData(fileName = "data//100//Linear100.csv");
resultsf <- data.frame(matrix(data=NA,nrow=8,ncol=7));
colnames(resultsf) <- c("Train_STD_Residuals","Train_R_Squared","Validation_RMSE", "Validation_MAPD");
modelFit_1$df.residual
modelFit_1$residuals
rowResults <- c(1,2,3,4)
bind(resultsf,rowResults)
cbind(resultsf,rowResults)
rbind(resultsf,rowResults)
computeErrors <- function(training_error,predicted_values,actual_values){
training_rmse <- rmse(training_error);
training_rsquared <- r_squared(predicted_values,actual_values);
validation_rmse <- rmse(predicted_values-actual_values);
validation_mapd <- mapd(predicted_values,actual_values);
rowResults <- c(training_rmse,training_rsquared,validation_rmse,validation_mapd);
}
modelFit_1 <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY)  ,data=trainingData);
predicted_values <- predict(modelFit_1,validationData);
resultsf <- rbind(resultsf,computeErrors(modelFit_1$residuals,predicted_values,validationData$UTILITY_INCREASE));
resultsf
resultsf <- data.frame(matrix(data=NA,nrow=8,ncol=7));
colnames(resultsf) <- c("Train_RMSE","Train_R_Squared","Validation_RMSE", "Validation_MAPD");
resultsf <- rbind(resultsf,computeErrors(modelFit_1$residuals,predicted_values,validationData$UTILITY_INCREASE));
resultsf
resultsf <- data.frame(matrix(data=NA,nrow=8,ncol=4));
colnames(resultsf) <- c("Train_RMSE","Train_R_Squared","Validation_RMSE", "Validation_MAPD");
resultsf <- rbind(resultsf,computeErrors(modelFit_1$residuals,predicted_values,validationData$UTILITY_INCREASE));
resultsf
resultsf <- data.frame(matrix(data=NA,nrow=0,ncol=4));
colnames(resultsf) <- c("Train_RMSE","Train_R_Squared","Validation_RMSE", "Validation_MAPD");
resultsf <- rbind(resultsf,computeErrors(modelFit_1$residuals,predicted_values,validationData$UTILITY_INCREASE));
resultsf
resultsf <- data.frame(matrix(data=NA,nrow=1,ncol=4));
colnames(resultsf) <- c("Train_RMSE","Train_R_Squared","Validation_RMSE", "Validation_MAPD");
resultsf <- rbind(resultsf,computeErrors(modelFit_1$residuals,predicted_values,validationData$UTILITY_INCREASE));
resultsf
resultsf <- data.frame();#matrix(data=NA,nrow=1,ncol=4));
colnames(resultsf) <- c("Train_RMSE","Train_R_Squared","Validation_RMSE", "Validation_MAPD");
resultsf[1,]<- computeErrors(modelFit_1$residuals,predicted_values,validationData$UTILITY_INCREASE));
resultsf[1,]<- computeErrors(modelFit_1$residuals,predicted_values,validationData$UTILITY_INCREASE);
resultsf <- data.frame(matrix(data=NA,nrow=1,ncol=4));
resultsf[1,]<- computeErrors(modelFit_1$residuals,predicted_values,validationData$UTILITY_INCREASE);
resultsf <- data.frame(matrix(data=NA,nrow=1,ncol=4));
colnames(resultsf) <- c("Train_RMSE","Train_R_Squared","Validation_RMSE", "Validation_MAPD");
resultsf[1,]<- computeErrors(modelFit_1$residuals,predicted_values,validationData$UTILITY_INCREASE);
resultsf
resultsf <- data.frame(matrix(data=NA,nrow=8,ncol=4));
colnames(resultsf) <- c("Train_RMSE","Train_R_Squared","Validation_RMSE", "Validation_MAPD");
modelFit <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY)  ,data=trainingData);
#summary(modelFit_1);
predicted_values <- predict(modelFit,validationData);
resultsf[1,]<- computeErrors(modelFit$residuals,predicted_values,validationData$UTILITY_INCREASE);
modelFit <- lm(UTILITY_INCREASE ~ CONNECTIVITY*CRITICALITY + RELIABILITY ,data=trainingData);
predicted_values <- predict(modelFit,validationData);
resultsf[2,]<- computeErrors(modelFit$residuals,predicted_values,validationData$UTILITY_INCREASE);
modelFit <- lm(UTILITY_INCREASE ~ CRITICALITY ,data=trainingData);
predicted_values <- predict(modelFit,validationData);
resultsf[3,]<- computeErrors(modelFit$residuals,predicted_values,validationData$UTILITY_INCREASE);
modelFit <- lm(UTILITY_INCREASE ~ CONNECTIVITY ,data=trainingData);
predicted_values <- predict(modelFit,validationData);
resultsf[4,]<- computeErrors(modelFit$residuals,predicted_values,validationData$UTILITY_INCREASE);
modelFit<- lm(UTILITY_INCREASE ~ RELIABILITY ,data=trainingData);
predicted_values <- predict(modelFit,validationData);
resultsf[5,]<- computeErrors(modelFit$residuals,predicted_values,validationData$UTILITY_INCREASE);
modelFit <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY) +log(RELIABILITY),data=trainingData);
predicted_values <- predict(modelFit,validationData);
resultsf[6,]<- computeErrors(modelFit$residuals,predicted_values,validationData$UTILITY_INCREASE);
modelFit <- lm(UTILITY_INCREASE ~ CRITICALITY*CONNECTIVITY*RELIABILITY ,data=trainingData);
predicted_values <- predict(modelFit,validationData);
resultsf[7,]<- computeErrors(modelFit$residuals,predicted_values,validationData$UTILITY_INCREASE);
modelFit <- lm(UTILITY_INCREASE ~ . ,data=trainingData);
predicted_values <- predict(modelFit,validationData);
resultsf[8,]<- computeErrors(modelFit$residuals,predicted_values,validationData$UTILITY_INCREASE);
modelFit <- lm(UTILITY_INCREASE ~ CRITICALITY*CONNECTIVITY*RELIABILITY ,data=trainingData);
predicted_values <- predict(modelFit,validationData);
summary(modelFit)
modelFit <- lm(UTILITY_INCREASE ~ . ,data=trainingData);
summary(modelFit)
modelFit <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY) +log(RELIABILITY),data=trainingData);
summary(modelFit)
resultsf
modelFit <- lm(UTILITY_INCREASE ~ CONNECTIVITY*CRITICALITY + RELIABILITY ,data=trainingData);
predicted_values <- predict(modelFit,validationData);
modelFit <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY) +log(RELIABILITY),data=trainingData);
predicted_values <- predict(modelFit,log(validationData));
resultsf[6,]<- computeErrors(modelFit$residuals, predicted_values,log(validationData$UTILITY_INCREASE));
resultsf
modelFit <- lm(log(UTILITY_INCREASE) ~ log(CRITICALITY) + log(CONNECTIVITY)  ,data=trainingData);
predicted_values <- predict(modelFit,log(validationData));
resultsf[1,]<- computeErrors(modelFit$residuals,predicted_values,log(validationData$UTILITY_INCREASE));
resultsf
modelFit <- lm(UTILITY_INCREASE ~ CONNECTIVITY*CRITICALITY + RELIABILITY ,data=trainingData);
predicted_values <- predict(modelFit,validationData);
resultsf[2,]<- computeErrors(modelFit$residuals,predicted_values,validationData$UTILITY_INCREASE);
modelFit_CCplusR.lm <- modelFit; #Save it to export
modelFit <- lm(UTILITY_INCREASE ~ CRITICALITY*CONNECTIVITY*RELIABILITY ,data=trainingData);
predicted_values <- predict(modelFit,validationData);
resultsf[7,]<- computeErrors(modelFit$residuals,predicted_values,validationData$UTILITY_INCREASE);
modelFit_CCR.lm <- modelFit; #Save it to export
resultsf
r2pmml(modelFit_CCplusR.lm, "CriticalityConnectivity_plus_Reliability_LM.pmml")
r2pmml(modelFit_CCR.lm, "CriticalityConnectivityReliability_LM.pmml")
r2pmml(modelFit_CCplusR.lm, "CriticalityConnectivity_plus_Reliability_LM.pmml")
library(XML)
r2pmml(modelFit_CCplusR.lm, "CriticalityConnectivity_plus_Reliability_LM.pmml")
library("devtools")
library(r2pmml)
library(XML)
r2pmml(modelFit_CCplusR.lm, "CriticalityConnectivity_plus_Reliability_LM.pmml")
r2pmml(modelFit_CCplusR.lm, "CriticalityConnectivity_plus_Reliability_LM.pmml")
r2pmml(modelFit_CCplusR.lm, "models//CriticalityConnectivity_plus_Reliability_LM.pmml")
r2pmml(modelFit_CCR.lm, "models//CriticalityConnectivityReliability_LM.pmml")
