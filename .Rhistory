#Load utility functions
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
library(xgboost)
library(r2pmml) #https://github.com/jpmml/r2pmml
a <- {1.4, 2.5, 3.1, 5.5}
a <- c(1.4, 2.5, 3.1, 5.5)
b <- c(2.1, 3.9, 4.3, 6.7)
sd(a)
sd(b)
1/2*100
100 * 1/2
100 * 2/3
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
#Download and install the latest H2O package for R.
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))
library(h2o)
h2o.init()
demo(h2o.kmeans)
install.packages("gbm")
install.packages("devtools")
install_git("git://github.com/jpmml/r2pmml.git")
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//DataPoints_1K-3K-9K//";
modelList <- c("Linear","Discontinuous","Saturating","ALL");
modelName <- modelList[1];
datasetSize <- c("1K","3K","9K");
datasetName <- generateDataSetNames(modelName,datasetSize,0);
# Generate the dataset names that will be trained ----------------------------
generateDataSetNames <- function(modelName,datasetSize,s_idx){
if(s_idx==0 & length(datasetSize)>0){#Generate for all sizes
datasetName <- paste0(modelName,datasetSize[1]);
for(i in c(2:length(datasetSize))){
datasetName <- cbind(datasetName,paste0(modelName,datasetSize[i]));
}
}
else{
datasetName <- paste0(modelName,datasetSize[s_idx]);
}
return(datasetName);
}
# Save results to file ----------------------------------------------------
resultsToFile <- function(mcResults,modelName,extension){
fileName <- paste0("mcResultsf_",modelName,extension);
write.table(mcResults,fileName,sep=",",col.names = TRUE);
print(paste0("file written:",fileName));
mcResults
}
# Prepare features --------------------------------------------------------
prepareFeatures <- function(dataf,selectionType){
#Do feature selection (or not)
if(selectionType=="ALL")
featuresdf<- select_ALL(dataf)
else
if(selectionType=="Linear")
featuresdf<- select_Linear(dataf)
else
if(selectionType=="Discontinuous")
featuresdf<- select_Discontinuous(dataf)
else
if(selectionType=="Saturating")
featuresdf<- select_Saturation(dataf)
#Remove zero utilities
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
# Scramble data
featuresdf <- scrambleData(datadf=featuresdf);
return (featuresdf);
}
library(devtools)
library(xgboost)
library(lightgbm, quietly=TRUE)
library(r2pmml) #https://github.com/jpmml/r2pmml
library(lgbm)
devtools::install_github("Laurae2/lgbdl", force = TRUE)
lgb.dl(commit = "master",
compiler = "vs",
repo = "https://github.com/Microsoft/LightGBM")
library(lightgbm)
install_github("Microsoft/LightGBM", subdir = "R-package")
install.packages("devtools")
library(devtools)
install_github("Microsoft/LightGBM", subdir = "R-package")
library(devtools)
options(devtools.install.args = "--no-multiarch") # if you have 64-bit R only, you can skip this
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
library(lightgbm)
data(agaricus.train, package='lightgbm')
train <- agaricus.train
dtrain <- lgb.Dataset(train$data, label=train$label)
params <- list(objective="regression", metric="l2")
model <- lgb.cv(params, dtrain, 10, nfold=5, min_data=1, learning_rate=1, early_stopping_rounds=10)
## Control code to run Random Fores
#https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/
library(randomForest)
library(caret)
library(devtools)
library(r2pmml) #https://github.com/jpmml/r2pmml
# Initialization section ------------------------------------------------------
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//DataPoints_1K-3K-9K//";
#folder <- "//DataPoints_1K-3K-9K//";
model.name.list <- c("Linear","Discontinuous","Saturating","ALL");
model.name <- model.name.list[2];
method.name <- "RF";
dataset.name.list <- generateDataSetNames(model.name, c("1K","2K","9K"),3);
results.df <- data.frame(matrix(data=NA,nrow=1000,ncol=14));
colnames(results.df) <- c("Item","Utility_Type","RMSE","R_Squared", "MADP","User_Time","Sys_Time","Elapsed_Time",
"Number_of_Trees","Learning_Rate","Max_Depth","Train_Split","Min_Data_In_Leaf","Bagging_Fraction");
results_line <- 0;
library(devtools)
library(xgboost)
library(r2pmml) #https://github.com/jpmml/r2pmml
# Initialization section ------------------------------------------------------
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//models//loadData.R");
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//models//xboostRegression.R");
#Data structure to keep results
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//";
datafolder_1k3k9k <- "//DataPoints_1K-3K-9K//";
datafolder_10K <- "//DataPoints_10K-150K//";
folder <- paste0(folder,datafolder_10K);
# CONTROL CODE   ---------------------------------------------------------------
model.name.list <- c("Linear","Discontinuous","Saturating","Combined");
model.name <- model.name.list[4];
method.name <- "XGB";
dataset.name.list <- generateDataSetNames(model.name, c("1K","3K","9K"),0);
results.df <- data.frame(matrix(data=NA,nrow=1000,ncol=7));
colnames(results.df) <- c("Item","Utility_Type","RMSE","R_Squared", "MADP","Elapsed_Time","Number_of_Trees");
#                        "User_Time","Sys_Time","Learning_Rate","Max_Depth","Train_Split","Min_Data_In_Leaf",
#                        "Bagging_Fraction");
results_line <- 0;
for(model.name in model.name.list){
dataset.name.list <- generateDataSetNames(model.name, c("1K","3K","9K"),0);
for(i in c(1:length(dataset.name.list))){
#i <- 1;
results_line <- results_line+1;
fileName <- paste0(folder,dataset.name.list[i],".csv");
data.df <- loadData(fileName);
features.df <- prepareFeatures(data.df,"Combined");
#Extract training and validation sets
totalData.size <- dim(features.df)[1];
training.size <- trunc(totalData.size * 0.7);
training.df <- as.data.frame(features.df[1:training.size-1,]);
validation.df <- as.data.frame(features.df[training.size:totalData.size,]);
#Train model
numberOfTrees <- 500;
kfolds <- 10;
outcome.list <- train_XGBoost(training.df,numberOfTrees,kfolds);
dataset.name.list[i];
#Validate model
results.df <- validate_XGBoost(outcome.list,validation.df,dataset.name.list[i],results_line,results.df);
}
}
library(devtools)
library(xgboost)
library(r2pmml) #https://github.com/jpmml/r2pmml
# Initialization section ------------------------------------------------------
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//models//loadData.R");
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//models//xboostRegression.R");
#Data structure to keep results
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//";
datafolder_1k3k9k <- "//DataPoints_1K-3K-9K//";
#datafolder_10K <- "//DataPoints_10K-150K//";
folder <- paste0(folder,datafolder_1k3k9k);
# CONTROL CODE   ---------------------------------------------------------------
model.name.list <- c("Linear","Discontinuous","Saturating","Combined");
model.name <- model.name.list[4];
method.name <- "XGB";
dataset.name.list <- generateDataSetNames(model.name, c("1K","3K","9K"),0);
results.df <- data.frame(matrix(data=NA,nrow=1000,ncol=7));
colnames(results.df) <- c("Item","Utility_Type","RMSE","R_Squared", "MADP","Elapsed_Time","Number_of_Trees");
#                        "User_Time","Sys_Time","Learning_Rate","Max_Depth","Train_Split","Min_Data_In_Leaf",
#                        "Bagging_Fraction");
results_line <- 0;
for(model.name in model.name.list){
dataset.name.list <- generateDataSetNames(model.name, c("1K","3K","9K"),0);
for(i in c(1:length(dataset.name.list))){
#i <- 1;
results_line <- results_line+1;
fileName <- paste0(folder,dataset.name.list[i],".csv");
data.df <- loadData(fileName);
features.df <- prepareFeatures(data.df,"Combined");
#Extract training and validation sets
totalData.size <- dim(features.df)[1];
training.size <- trunc(totalData.size * 0.7);
training.df <- as.data.frame(features.df[1:training.size-1,]);
validation.df <- as.data.frame(features.df[training.size:totalData.size,]);
#Train model
numberOfTrees <- 500;
kfolds <- 10;
outcome.list <- train_XGBoost(training.df,numberOfTrees,kfolds);
dataset.name.list[i];
#Validate model
results.df <- validate_XGBoost(outcome.list,validation.df,dataset.name.list[i],results_line,results.df);
}
}
for(model.name in model.name.list){
dataset.name.list <- generateDataSetNames(model.name, c("1K","3K","9K"),0);
for(i in c(1:length(dataset.name.list))){
#i <- 1;
results_line <- results_line+1;
fileName <- paste0(folder,dataset.name.list[i],".csv");
data.df <- loadData(fileName);
features.df <- prepareFeatures(data.df,"Combined");
#Extract training and validation sets
totalData.size <- dim(features.df)[1];
training.size <- trunc(totalData.size * 0.7);
training.df <- as.data.frame(features.df[1:training.size-1,]);
validation.df <- as.data.frame(features.df[training.size:totalData.size,]);
#Train model
numberOfTrees <- 1000;
kfolds <- 10;
outcome.list <- train_XGBoost(training.df,numberOfTrees,kfolds);
dataset.name.list[i];
#Validate model
results.df <- validate_XGBoost(outcome.list,validation.df,dataset.name.list[i],results_line,results.df);
}
}
message <- resultsToFile(results.df,model.name,method.name,"XGB_70-30_500_trees.csv"); #save to a .csv file
print(message);
