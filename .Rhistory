trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgb.model
xgb.test.data = xgb.DMatrix(data.matrix(validationData[,1:7]), missing = NA)
xgb.preds = predict(xgb.model, xgb.test.data)
xgb.roc_obj <- roc(validationData[,"Utility.Increase"], xgb.preds)
xgb.roc_obj
# Call:
#   roc.default(response = validationData[, "Utility.Increase"],     predictor = xgb.preds)
#
# Data: xgb.preds in 399 controls (validationData[, "Utility.Increase"] 0) > 1 cases
#(validationData[, "Utility.Increase"] 13.3018695).
# Area under the curve: 0.8571
# Metric functions --------------------------------------------------------
# Root mean square error
# https://en.wikipedia.org/wiki/Root-mean-square_deviation
rmse <- function(error){
sqrt(mean(error^2))
}
# Coefficient of determination
# https://en.wikipedia.org/wiki/Coefficient_of_determination
r_squared <- function(prediction, actual){
SS_ExplainedVariance <- sum((prediction - actual)^2);
SS_TotalVariance <- sum((actual-mean(actual))^2);
R2<- 1- SS_ExplainedVariance / SS_TotalVariance;
return (R2);
}
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility.Increase;
plot(error,y_pred)
plot(y_pred,error)
plot(xgb.model)
gbtest(model)
library(lmtest)
install.packages("lmtest")
library(lmtest)
model <- lm(y_pred ~ Provided.Interface, validationData)
gbtest(model)
bptest(model)
gvlma(model)
library(gvlma)
gvlma(model)
bptest(xgb.model)
bptest(xgb.preds)
resid(xgb.model)
hist(error)
qqnorm(error,main="Normal Q-Q Plot - error ")
qqline(error)
shapiro.test(error)
refError <- error/validationData$Utility.Increase
plot(refError) + title(main="Relative error")
meanError <- mean(y_pred - validationData$Utility.Increase)
percentMeanError <- meanError / mean(validationData$Utility.Increase)*100;
percentMeanError
rmse_value <- rmse(error);
rmse_value
prediction <- data.frame(y_pred);
actual <- data.frame(validationData$Utility.Increase);
R2 <- r_squared(prediction,as.numeric(validationData$Utility.Increase));
R2
errorFrame <- data.frame(validationData);
errorFrame$residuals<- error;
head(errorFrame)
plot(errorFrame) #shows that the errors are randomly distributed for range of the predictions
errorFrame<-errorFrame[-Utility.Increase,]
errorFrame<-errorFrame[-"Utility.Increase",]
errorFrame <- data.frame(validationData[1:7,]);
errorFrame$residuals<- error;
errorFrame <- data.frame(validationData[,1:7]);
errorFrame$residuals<- error;
errorFrame<-errorFrame["Utility.Increase",]
errorFrame <- data.frame(validationData[,1:7]);
errorFrame$residuals<- error;
plot(errorFrame[1:500,]) #shows that the errors are randomly distributed for range of the predictions
?for
for loop
?forlop
?forloop
?for loop
for(i in 1:7){
plot(error,errorFrame[,i]) #shows that the errors are randomly distributed for range of the predictions
}
for(i in 1:7){
plot(errorFrame[,i],error) #shows that the errors are randomly distributed for range of the predictions
}
xgb.model
summary(xgb.model)
resultsf <- data.frame();
colnames(resltsf) <- c("Training_Error","Training_STD","Testing_Error", "Testing_STD","RMSE","R_Squared");
resultsf <- data.frame();
colnames(resultsf) <- c("Training_Error","Training_STD","Testing_Error", "Testing_STD","RMSE","R_Squared");
colnames(resultsf) <- c("Training_Error","Training_STD","Testing_Error", "Testing_STD","RMSE","R_Squared");
resultsf <- data.frame(as.matrix.data.frame());
resultsf <- data.frame(as.matrix.data.frame(c(1:100)));
resultsf <- data.frame(as.matrix.data.frame(c(1:100,1:4)));
resultsf <- data.frame(as.matrix.data.frame(c(1:100),c(1:4)));
resultsf <- data.frame(as.matrix.data.frame());
?as.matrix.data.frame
resultsf <- data.frame(as.matrix.data.frame(data=NA,nrow=100,ncol=6));
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=6));
colnames(resultsf) <- c("Training_Error","Training_STD","Testing_Error", "Testing_STD","RMSE","R_Squared");
head(resultsf)
x <- seq(1,10,length.out = 100)        # our predictor
plot(x)
hist(x)
sigmas <- seq(0.5,20,length.out = 20)
y <- 2 + 1.2*x + rnorm(100,0,sd = sig)
y <- 2 + 1.2*x + rnorm(100,0,sd = sigmas)
plot(y)
hist(y)
model <- lm(y ~ x)
model
plot(model)
model$r.squared
summary(model)
install.packages("Hmisc")
library("Hmisc")
# load data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<-loadData(fileName="data//Linear.csv");
qqplot(dataf$CONNECTIVITY)
qqnorm(dataf$CRITICALITY,main="Normal Q-Q Plot - Criticality")+ qqline();
qqline(dataf$CRITICALITY)
dataf<-loadData(fileName="data//Linear.csv");
qqnorm(dataf$CRITICALITY,main="Normal Q-Q Plot - Criticality")+ qqline();
qqline(dataf$CRITICALITY)
res <- rcorr(data.matrix(matrixInput),type=c("spearman"));
matrixInput<-data.frame(dataf$UTILITY.INCREASE,dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,
dataf$IMPORTANCE, dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,dataf$ADT);
colnames(matrixInput)<-c("Utility.Increase","Criticality","Connectivity","Reliability","Importance",
"Provided.Inteface","Required.Interface","ADT");
res <- rcorr(data.matrix(matrixInput),type=c("spearman"));
res$r
res$P
res <- rcorr(data.matrix(matrixInput),type=c("pearson"));
res$r
res$P
dataf<-loadData(fileName="data//Probabilistic.csv");
matrixInput<-data.frame(dataf$UTILITY.INCREASE,dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,
dataf$IMPORTANCE, dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,dataf$ADT);
colnames(matrixInput)<-c("Utility.Increase","Criticality","Connectivity","Reliability","Importance",
"Provided.Inteface","Required.Interface","ADT");
res <- rcorr(data.matrix(matrixInput),type=c("pearson"));
res$r
res$P
dataf<-loadData(fileName="data//Probabilistic.csv");
matrixInput<-data.frame(dataf$UTILITY.INCREASE,dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY,
dataf$IMPORTANCE, dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE,dataf$ADT);
colnames(matrixInput)<-c("Utility.Increase","Criticality","Connectivity","Reliability","Importance",
"Provided.Inteface","Required.Interface","ADT");
res <- rcorr(data.matrix(matrixInput),type=c("pearson"));
res$r
res$P
dataf<- dataf[dataf$RELIABILITY!=0,];
dataf <- dataf[dataf$UTILITY.INCREASE!=0,]
xgboost.cv$evaluation_log[best_iteration]
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=6));
colnames(resultsf) <- c("Training_RMSE","Training_STD","Testing_RMSE", "Testing_STD","RMSE","R_Squared");
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN", "Test_RMSE_STD","RMSE","R_Squared");
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=6));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN", "Test_RMSE_STD","RMSE","R_Squared");
library(xgboost)
library(pROC)
# load data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<-loadData(fileName="data//Linear.csv");
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=6));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN", "Test_RMSE_STD","RMSE","R_Squared");
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE, dataf$ADT, dataf$UTILITY.INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance","Provided.Interface",
"Required.Interface","ADT","Utility.Increase");
proportion <- 0.8
for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility.Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility.Increase);
}
plot(resultsf$Train_RMSE_MEAN, main="Training Error, 80/20");
mean(resultsf$Train_RMSE_MEAN)
plot(resultsf$Test_RMSE_MEAN, main="Training Error, 80/20");
mean(resultsf$Test_RMSE_MEAN)
mean(resultsf$RMSE)
plot(resultsf$RMSE, main="Validatoin RMSE, 80/20, mean=4.471624");
mean(resultsf$R_Squared)
plot(resultsf$R_Squared, main="Validatoin RMSE, 80/20, mean=0.9917632");
proportion <- 0.7
# load data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf<-loadData(fileName="data//Linear.csv");
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=6));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN", "Test_RMSE_STD","RMSE","R_Squared");
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE, dataf$ADT, dataf$UTILITY.INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance","Provided.Interface",
"Required.Interface","ADT","Utility.Increase");
proportion <- 0.7
for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility.Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility.Increase);
}
plot(resultsf$Train_RMSE_MEAN, main="Training RMSE, 70/30, mean=2.809366");
mean(resultsf$Train_RMSE_MEAN)
mean(resultsf$Test_RMSE_MEAN)
mean(resultsf$RMSE)
mean(resultsf$R_Squared)
plot(resultsf$Train_RMSE_MEAN, main="Training RMSE, 70/30, mean=2.717048");
plot(resultsf$Test_RMSE_MEAN, main="Testing RMSE, 70/30, mean= 7.161525");
plot(resultsf$RMSE, main="Validatoin RMSE, 70/30, mean=5.360206");
plot(resultsf$R_Squared, main="Validatoin R_Squared, 70/30, mean=0.9881726");
plot(resultsf$Train_RMSE_MEAN, main="Training RMSE, 70/30, mean=2.717048");
mean(resultsf$Train_RMSE_MEAN)
plot(resultsf$Test_RMSE_MEAN, main="Testing RMSE, 70/30, mean= 7.161525");
mean(resultsf$Test_RMSE_MEAN)
plot(resultsf$RMSE, main="Validation RMSE, 70/30, mean=5.360206");
mean(resultsf$RMSE)
plot(resultsf$R_Squared, main="Validation R_Squared, 70/30, mean=0.9881726");
mean(resultsf$R_Squared)
dataf<-loadData(fileName="data//Probabilistic.csv");
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=6));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN", "Test_RMSE_STD","RMSE","R_Squared");
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE, dataf$ADT, dataf$UTILITY.INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance","Provided.Interface",
"Required.Interface","ADT","Utility.Increase");
proportion <- 0.7
for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility.Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility.Increase);
}
mean(resultsf$Train_RMSE_MEAN)
mean(resultsf$Test_RMSE_MEAN)
mean(resultsf$RMSE)
mean(resultsf$R_Squared)
plot(resultsf$Train_RMSE_MEAN, main="Training RMSE, 70/30, mean= 2.655605");
plot(resultsf$Test_RMSE_MEAN, main="Testing RMSE, 70/30, mean= 8.135508");
plot(resultsf$RMSE, main="Validation RMSE, 70/30, mean=5.927283");
plot(resultsf$R_Squared, main="Validation R_Squared, 70/30, mean=0.9860166");
dataf<-loadData(fileName="data//discontinous.csv");
dataf<-loadData(fileName="data//discontinous.csv");
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=6));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN", "Test_RMSE_STD","RMSE","R_Squared");
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE, dataf$ADT, dataf$UTILITY.INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance","Provided.Interface",
"Required.Interface","ADT","Utility.Increase");
proportion <- 0.7
for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility.Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility.Increase);
}
mean(resultsf$Train_RMSE_MEAN)
mean(resultsf$Test_RMSE_MEAN)
mean(resultsf$RMSE)
mean(resultsf$R_Squared)
plot(resultsf$Train_RMSE_MEAN, main="Training RMSE, 70/30, mean= 16.42466");
plot(resultsf$Test_RMSE_MEAN, main="Testing RMSE, 70/30, mean= 120.5871");
plot(resultsf$RMSE, main="Validation RMSE, 70/30, mean=80.52817");
plot(resultsf$R_Squared, main="Validation R_Squared, 70/30, mean=0.9905962");
hist(resultsf$RMSE)
resultsf$RMSE/validationData$Utility.Increase
resultsf$RMSE/(mean(validationData$Utility.Increase)
mean(resultsf$RMSE)/(mean(validationData$Utility.Increase)
mean(resultsf$RMSE)/mean(validationData$Utility.Increase)
dataf<-loadData(fileName="data//Linear.csv");
proportion <- 0.7
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
rmseLinear <- 5.360602
meanLinear <- mean(validationData$Utility.Increase)
rmseLinear/meanLinear;
rmseLinear/meanLinear *100
dataf<-loadData(fileName="data//Probabilistic.csv");
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
meanLinear <- mean(validationData$Utility.Increase)
rmseLinear <- 5.97283
rmseLinear/meanLinear *100
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=6));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN", "Test_RMSE_STD","RMSE","R_Squared");
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE, dataf$ADT, dataf$UTILITY.INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance","Provided.Interface",
"Required.Interface","ADT","Utility.Increase");
proportion <- 0.7
for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility.Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility.Increase);
}
hist(resultsf$RMSE)
dataf<-loadData(fileName="data//Linear.csv");
resultsf <- data.frame(matrix(data=NA,nrow=100,ncol=6));
colnames(resultsf) <- c("Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN", "Test_RMSE_STD","RMSE","R_Squared");
# Select feature columns --------------------------------------------------
featuresdf<- data.frame(dataf$CRITICALITY,dataf$CONNECTIVITY,dataf$RELIABILITY, dataf$IMPORTANCE,
dataf$PROVIDED_INTERFACE, dataf$REQUIRED_INTERFACE, dataf$ADT, dataf$UTILITY.INCREASE);
colnames(featuresdf) <- c("Criticality","Connectivity","Reliability","Importance","Provided.Interface",
"Required.Interface","ADT","Utility.Increase");
proportion <- 0.7
for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(dataf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:7]),
label = trainingData[,"Utility.Increase"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse')
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$Utility.Increase;
resultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
resultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
resultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
resultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
resultsf$RMSE[i] <- rmse(error);
resultsf$R_Squared[i] <- r_squared(y_pred,validationData$Utility.Increase);
}
hist(resultsf$RMSE)
dataf<-loadData(fileName="data//Linear.csv");
dataf<-loadData(fileName="data//Probabilistic.csv");
dataf_l<-loadData(fileName="data//Linear.csv");
dataf_p<-loadData(fileName="data//Probabilistic.csv");
plot(dataf_l$UTILITY.INCREASE,dataf_p$UTILITY.INCREASE)
mean(dataf_l$UTILITY.INCREASE)
mean(dataf_p$UTILITY.INCREASE)
stderr(dataf_l$UTILITY.INCREASE)
stdv(dataf_l$UTILITY.INCREASE)
hist(dataf_l$UTILITY.INCREASE)
hist(dataf_p$UTILITY.INCREASE)
sd
?sd
sd(dataf_l$UTILITY.INCREASE)
sd(dataf_p$UTILITY.INCREASE)
