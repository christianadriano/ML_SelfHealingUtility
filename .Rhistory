# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:tweedie", base_score = 0.5)# booster="gbtree")
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse',verbose = FALSE)
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Generate PMML file ------------------------------------------------------
# Generate feature map
mpg.fmap = r2pmml::genFMap(featuresdf)
r2pmml::writeFMap(mpg.fmap, "xgboost.fmap")
# Save the model in XGBoost proprietary binary format
xgb.save(xgb.model, "xgboost.model")
# Dump the model in text format
xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap")
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$UTILITY_INCREASE;
mcResultsf$DataSet[i]<-datasetName[i];
mcResultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
mcResultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
mcResultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
mcResultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
mcResultsf$RMSE[i] <- rmse(error);
mcResultsf$R_Squared[i] <- r_squared(y_pred,validationData$UTILITY_INCREASE);
mcResultsf$MAPD[i] <- mapd(y_pred,validationData$UTILITY_INCREASE);
mcResultsf
events = paste("events =",dim(dataf)[1]);
plot(y_pred, type="p",col="red", pch=4, xlab=events, ylab = "Utility Increase")
points(validationData$UTILITY_INCREASE)
title = paste("Pred (red cross) x Actual,", "All Components",", MAPD =", mcResultsf$MAPD[i],"%");
title(title);
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
dataf <- loadData(fileName);
dim(dataf)
# Select feature columns --------------------------------------------------
# featuresdf<- select_Linear(dataf)
# featuresdf<- select_Probabilistic(dataf)
# featuresdf<- select_Discontinous(dataf)
# featuresdf<- select_Saturation(dataf)
featuresdf<- select_ALL(dataf)
inputFeatures <- dim(featuresdf)[2] - 1;
#RUN ALL DATA SETS WITH ALL FEATURES TO CHECK IF THE MODEL IS ABLE TO GET RID OF
#USELESS FEATURES.
#averageResultsf <- data.frame(matrix(data=NA,nrow=6,ncol=8));
#colnames(resultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
#                        "Test_RMSE_STD","RMSE","R_Squared", "MAPD");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
featuresdf <- featuresdf[featuresdf$ADT!=0,];
#for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(datadf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:tweedie", base_score = 0.5)# booster="gbtree")
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse',verbose = FALSE)
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Generate PMML file ------------------------------------------------------
# Generate feature map
mpg.fmap = r2pmml::genFMap(featuresdf)
r2pmml::writeFMap(mpg.fmap, "xgboost.fmap")
# Save the model in XGBoost proprietary binary format
xgb.save(xgb.model, "xgboost.model")
# Dump the model in text format
xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap")
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$UTILITY_INCREASE;
mcResultsf$DataSet[i]<-datasetName[i];
mcResultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
mcResultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
mcResultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
mcResultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
mcResultsf$RMSE[i] <- rmse(error);
mcResultsf$R_Squared[i] <- r_squared(y_pred,validationData$UTILITY_INCREASE);
mcResultsf$MAPD[i] <- mapd(y_pred,validationData$UTILITY_INCREASE);
events = paste("events =",dim(dataf)[1]);
plot(y_pred, type="p",col="red", pch=4, xlab=events, ylab = "Utility Increase")
points(validationData$UTILITY_INCREASE)
title = paste("Pred (red cross) x Actual,", affectedComponent[i],", ",name,", MAPD =", mcResultsf$MAPD[i],"%");
title = paste("Pred (red cross) x Actual,", "All Components",", MAPD =", mcResultsf$MAPD[i],"%");
title(title);
name = "Saturating10K";
fileName = paste0(folder,name,".csv");
i=3;
dataf <- loadData(fileName);
dim(dataf)
# Select feature columns --------------------------------------------------
# featuresdf<- select_Linear(dataf)
# featuresdf<- select_Probabilistic(dataf)
# featuresdf<- select_Discontinous(dataf)
# featuresdf<- select_Saturation(dataf)
featuresdf<- select_ALL(dataf)
inputFeatures <- dim(featuresdf)[2] - 1;
#RUN ALL DATA SETS WITH ALL FEATURES TO CHECK IF THE MODEL IS ABLE TO GET RID OF
#USELESS FEATURES.
#averageResultsf <- data.frame(matrix(data=NA,nrow=6,ncol=8));
#colnames(resultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
#                        "Test_RMSE_STD","RMSE","R_Squared", "MAPD");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
featuresdf <- featuresdf[featuresdf$ADT!=0,];
#for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(datadf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:tweedie", base_score = 0.5)# booster="gbtree")
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse',verbose = FALSE)
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Generate PMML file ------------------------------------------------------
# Generate feature map
mpg.fmap = r2pmml::genFMap(featuresdf)
r2pmml::writeFMap(mpg.fmap, "xgboost.fmap")
# Save the model in XGBoost proprietary binary format
xgb.save(xgb.model, "xgboost.model")
# Dump the model in text format
xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap")
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$UTILITY_INCREASE;
mcResultsf$DataSet[i]<-datasetName[i];
mcResultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
mcResultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
mcResultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
mcResultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
mcResultsf$RMSE[i] <- rmse(error);
mcResultsf$R_Squared[i] <- r_squared(y_pred,validationData$UTILITY_INCREASE);
mcResultsf$MAPD[i] <- mapd(y_pred,validationData$UTILITY_INCREASE);
events = paste("events =",dim(dataf)[1]);
plot(y_pred, type="p",col="red", pch=4, xlab=events, ylab = "Utility Increase")
points(validationData$UTILITY_INCREASE)
title = paste("Pred (red cross) x Actual,", "All Components",", MAPD =", mcResultsf$MAPD[i],"%");
title(title);
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
name = "Saturating10K";
fileName = paste0(folder,name,".csv");
i=3;
dataf <- loadData(fileName);
dim(dataf)
# featuresdf<- select_Saturation(dataf)
featuresdf<- select_ALL(dataf)
inputFeatures <- dim(featuresdf)[2] - 1;
#RUN ALL DATA SETS WITH ALL FEATURES TO CHECK IF THE MODEL IS ABLE TO GET RID OF
#USELESS FEATURES.
#averageResultsf <- data.frame(matrix(data=NA,nrow=6,ncol=8));
#colnames(resultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
#                        "Test_RMSE_STD","RMSE","R_Squared", "MAPD");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
featuresdf <- featuresdf[featuresdf$ADT!=0,];
#for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(datadf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:tweedie", base_score = 0.5)# booster="gbtree")
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse',verbose = FALSE)
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Generate PMML file ------------------------------------------------------
# Generate feature map
mpg.fmap = r2pmml::genFMap(featuresdf)
r2pmml::writeFMap(mpg.fmap, "xgboost.fmap")
# Save the model in XGBoost proprietary binary format
xgb.save(xgb.model, "xgboost.model")
# Dump the model in text format
xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap")
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$UTILITY_INCREASE;
mcResultsf$DataSet[i]<-datasetName[i];
mcResultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
mcResultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
mcResultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
mcResultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
mcResultsf$RMSE[i] <- rmse(error);
mcResultsf$R_Squared[i] <- r_squared(y_pred,validationData$UTILITY_INCREASE);
mcResultsf$MAPD[i] <- mapd(y_pred,validationData$UTILITY_INCREASE);
events = paste("events =",dim(dataf)[1]);
plot(y_pred, type="p",col="red", pch=4, xlab=events, ylab = "Utility Increase")
points(validationData$UTILITY_INCREASE)
title = paste("Pred (red cross) x Actual,", "All Components",", MAPD =", mcResultsf$MAPD[i],"%");
title(title);
hist(error)
plot(error)
mean(error)
sd(error)
erro<-100
error <- y_pred - validationData$UTILITY_INCREASE;
erro<-100
error <- y_pred - validationData$UTILITY_INCREASE;
erro< -100
item<- erro< -100
item
plot(error[4000:4100])
dataf[4060:4063,]
dataf[4060:4063,1:4]
dataf[4060:4063,1:6]
plot(error[4000:4063])
plot(error[4060:4063])
plot(error[4060:4062])
dataf[4062:4063,1:6]
dataf[4062:4063,1:10]
dataf[4062,1:10]
dataf[4062,1:12]
dataf[4062,1:15]
dataf[4062:4063,1:15]
dataf[4062:4064,1:15]
plot(error[4060:4063])
plot(error[4050:4063])
plot(error[4000:4063])
plot(error[4000:4163])
plot(error[4000:4263])
plot(error[4200:4263])
plot(error[4200:4213])
plot(error[4209:4213])
plot(error[4212:4213])
plot(error[4211:4213])
plot(error[4211:4212])
dataf[4211,1:12]
dataf[4211,1:15]
index=0;
datasetSize="10K";
linear = paste0("Linear",datasetSize,".csv");
discontinous = paste0("Discontinous",datasetSize,".csv");
saturating = paste0("Saturating",datasetSize,".csv");
all = paste0("ALL",datasetSize,".csv");
datasetName <- c(linear,discontinous,saturating,all);
mcResultsf <- data.frame(matrix(data=NA,nrow=18,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
folder <- "data//New4Cases//";
dataf <- loadData(fileName=paste0(folder,all));
trainModel <- function(i, dataf,mcResultsf){
# Select feature columns --------------------------------------------------
# featuresdf<- select_Linear(dataf)
# featuresdf<- select_Probabilistic(dataf)
# featuresdf<- select_Discontinous(dataf)
# featuresdf<- select_Saturation(dataf)
featuresdf<- select_ALL(dataf)
inputFeatures <- dim(featuresdf)[2] - 1;
#RUN ALL DATA SETS WITH ALL FEATURES TO CHECK IF THE MODEL IS ABLE TO GET RID OF
#USELESS FEATURES.
#averageResultsf <- data.frame(matrix(data=NA,nrow=6,ncol=8));
#colnames(resultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
#                        "Test_RMSE_STD","RMSE","R_Squared", "MAPD");
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
featuresdf <- featuresdf[featuresdf$ADT!=0,];
#for(i in c(1:100)){
# Scramble data -----------------------------------------------------------
featuresdf <- scrambleData(datadf=featuresdf);
# Extract training ad validation sets -------------------------------------
#Training = used to create a model
#Validation = used to compute prediction error (Bias)
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
# Build model -------------------------------------------------------------
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)# booster="gbtree")
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse',verbose = FALSE)
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
# Generate PMML file ------------------------------------------------------
# Generate feature map
mpg.fmap = r2pmml::genFMap(featuresdf)
r2pmml::writeFMap(mpg.fmap, "xgboost.fmap")
# Save the model in XGBoost proprietary binary format
xgb.save(xgb.model, "xgboost.model")
# Dump the model in text format
xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap")
# Validation -------------------------------------------------------------
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$UTILITY_INCREASE;
mcResultsf$DataSet[i]<-datasetName[i];
mcResultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
mcResultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
mcResultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
mcResultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
mcResultsf$RMSE[i] <- rmse(error);
mcResultsf$R_Squared[i] <- r_squared(y_pred,validationData$UTILITY_INCREASE);
mcResultsf$MAPD[i] <- mapd(y_pred,validationData$UTILITY_INCREASE);
return(mcResultsf);
}
datasetName <- c("ALL100","ALL1000","ALL10K");
plot(dataf$PMax)
min(dataf$PMax)
max(dataf$PMax)
max(dataf$alpha)
min(dataf$alpha)
plot(dataf$alpha)
unique(dataf$alpha)
l <- unique(dataf$alpha)
length(l)
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
folder <- "data//New4Cases//";
datasetName <- c("ALL100","ALL1000","ALL10K");
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- load(fileName);
fileName <- paste0(folder,datasetName[i],".csv");
fileName
i
i=0
i=1
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- load(fileName);
fileName
property(fileName)
attributes(fileName)
dataf <- loadData(fileName);
mcResultsf <- trainModel(i,dataf,mcResultsf);
mcResultsf
for(i in c(1:length(datasetName))){
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
#Train model
mcResultsf <- trainModel(i,dataf,mcResultsf);
mcResultsf
}
mcResultsf
for(i in c(1:length(datasetName))){
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
#Train model
mcResultsf <- trainModel(i,dataf,mcResultsf);
mcResultsf
}
mcResultsf
datasetName <- c("Saturating100","Saturating1000","Saturating10K");
for(i in c(1:length(datasetName))){
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
#Train model
mcResultsf <- trainModel(i,dataf,mcResultsf);
mcResultsf
}
mcResultsf
datasetName <- c("Discontinous100","Discontinous1000","Discontinous10K");
for(i in c(1:length(datasetName))){
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
#Train model
mcResultsf <- trainModel(i,dataf,mcResultsf);
mcResultsf
}
mcResultsf
pmml(xgb.model)
library(r2pmml)
pmml(xgb.model)
library(pmml)
install.packages("pmml")
pmml
xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap")
pmml(xgb.model)
?pmml
library(pmmlTransformations)
library(pmml)
pmml(xgb.model)
xgb.map = r2pmml::genFMap(dataf)
r2pmml::writeFMap(xgb.map, "xgboost.fmap")
xgb.save(xgb.model, "xgboost.model")
xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap")
r2pmml(xgb.model, "xgb.pmml", fmap = xgboost.fmap, response_name = "UTILITY_INCREASE", missing = NULL, ntreelimit = 7, compact = TRUE)
xgboost.fmap = r2pmml::genFMap(dataf)
r2pmml::writeFMap(xgb.map, "xgboost.fmap")
xgb.save(xgb.model, "xgboost.model")
xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap")
r2pmml(xgb.model, "xgb.pmml", fmap = xgboost.fmap, response_name = "UTILITY_INCREASE", missing = NULL, ntreelimit = 7, compact = TRUE)
datasetName <- c("Saturating100","Saturating1000","Saturating10K");
i=3;
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
mcResultsf <- trainModel(i,dataf,mcResultsf);
mcResultsf
r2pmml(xgb.model, "xgb.pmml", fmap = xgboost.fmap, response_name = "UTILITY_INCREASE", missing = NULL, ntreelimit = 7, compact = TRUE)
featuresdf<- select_ALL(dataf)
inputFeatures <- dim(featuresdf)[2] - 1;
proportion <- 0.7
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
featuresdf <- featuresdf[featuresdf$ADT!=0,];
featuresdf <- scrambleData(datadf=featuresdf);
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * proportion);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)# booster="gbtree")
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 1500,
early_stopping_rounds = 100, metrics='rmse',verbose = FALSE)
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
xgboost.fmap = r2pmml::genFMap(featuresdf)
r2pmml::writeFMap(xgb.map, "xgboost.fmap")
xgb.save(xgb.model, "xgboost.model")
xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap")
datasetName <- c("Discontinous100","Discontinous1000","Discontinous10K");
i=1;
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
mcResultsf <- trainModel(i,dataf,mcResultsf);
mcResultsf
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
folder <- "data//New4Cases//";
datasetName <- c("Discontinous100","Discontinous1000","Discontinous10K");
i=1;
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
mcResultsf <- trainModel(i,dataf,mcResultsf);
mcResultsf
hist(dataf$UTILITY_INCREASE)
hist(dataf$UTILITY_INCREASE)
plot(dataf$UTILITY_INCREASE)
plot(dataf$UTILITY_INCREASE,dataf$REQUIRED_INTERFACE)
plot(dataf$UTILITY_INCREASE,dataf$PROVIDED_INTERFACE)
plot(dataf$UTILITY_INCREASE,dataf$REQUIRED_INTERFACE)
r2pmml(xgb.model, "xgb.pmml", fmap = xgboost.fmap, response_name = "UTILITY_INCREASE", missing = NULL, ntreelimit = 7, compact = TRUE)
pmmlFileName <- paste(folder,datasetName[i],"-xgb.pmml");
pmmlFileName <- paste(datasetName[i],"-xgb.pmml");
pmmlFileName <- paste0(datasetName[i],"-xgb.pmml");
pmmlFileName <- paste0(datasetName[i],"-xgb.pmml");
r2pmml(xgb.model, "xgb.pmml", fmap = xgboost.fmap, response_name = "UTILITY_INCREASE", missing = NULL, ntreelimit = 7, compact = TRUE)
datasetName <- c("Linear100","Linear1000","Linear10K");
i=3;
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
mcResultsf <- trainModel(i,dataf,mcResultsf);
mcResultsf
xgboost.fmap = r2pmml::genFMap(featuresdf)
r2pmml::writeFMap(xgb.map, "xgboost.fmap")
xgb.save(xgb.model, "xgboost.model")
xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap");
pmmlFileName <- paste0(datasetName[i],"-xgb.pmml");
r2pmml(xgb.model, "xgb.pmml", fmap = xgboost.fmap, response_name = "UTILITY_INCREASE", missing = NULL, ntreelimit = 7, compact = TRUE)
r2pmml(xgb.model, "xgb.pmml", fmap = xgboost.fmap, response_name = "UTILITY_INCREASE", missing = NULL, ntreelimit = 7, compact = TRUE)
r2pmml(xgb.model, pmmlFileName, fmap = xgboost.fmap, response_name = "UTILITY_INCREASE", missing = NULL, ntreelimit = 7, compact = TRUE)
datasetName <- c("Saturating100","Saturating1000","Saturating10K");
i=3;
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
mcResultsf <- trainModel(i,dataf,mcResultsf);
pmmlFileName <- paste0(datasetName[i],"-xgb.pmml");
xgboost.fmap = r2pmml::genFMap(featuresdf)
r2pmml::writeFMap(xgb.map, "xgboost.fmap")
# Save the model in XGBoost proprietary binary format
xgb.save(xgb.model, "xgboost.model")
pmmlFileName <- paste0(datasetName[i],"-xgb.pmml");
r2pmml(xgb.model, pmmlFileName, fmap = xgboost.fmap, response_name = "UTILITY_INCREASE", missing = NULL, ntreelimit = 7, compact = TRUE)
validationData
validationData[1:10,]
validationData[1:10,1:12]
validationData[1:10,1:5]
validationData[1:10,6:9]
validationData[1:10,10:12]
validationData[1:10,10:11]
