sampled_df
dim(sampled_df)
sampled_dataf<-data.frame();
questionID_f <- data.frame(unique(dataf$Question.ID));
colnames(questionID_f)<- c("id");
id=1;
questionSet<-dataf[dataf$Question.ID==id,]
sampled_df<- sample_n(questionSet, 5)
sampled_dataf<-rbind(sampled_dataf,sampled_df);
sampled_dataf
id=2;
questionSet<-dataf[dataf$Question.ID==id,]
sampled_df<- sample_n(questionSet, 5)
sampled_dataf<-rbind(sampled_dataf,sampled_df);
sampled_dataf
dim(sampled_dataf)
sampled_dataf<-data.frame();
questionID_f <- data.frame(unique(dataf$Question.ID));
colnames(questionID_f)<- c("id");
for(id in questionID_f$id){
questionSet<-dataf[dataf$Question.ID==id,]
sampled_df<- sample_n(questionSet, 5)
sampled_dataf<-rbind(sampled_dataf,sampled_df);
}
dim(sampled_dataf)
confidence_utility<-function(df){
subset_df<-subset(df,select=c(Answer.confidence))
#mark all rows that match the selection
conf.1<- rowSums(subset_df=="1");
subset_df["conf.1"] <- conf.1;
conf.2<- rowSums(subset_df=="2");
subset_df["conf.2"] <- conf.2;
conf.3<- rowSums(subset_df=="3");
subset_df["conf.3"] <- conf.3;
conf.4<- rowSums(subset_df=="4");
subset_df["conf.4"] <- conf.4;
conf.5<- rowSums(subset_df=="5");
subset_df["conf.5"] <- conf.5;
subset_df["QuestionID"] <- df$Question.ID;
subset_df <-subset(subset_df,select= c(QuestionID,conf.1,conf.2,conf.3,conf.4,conf.5));
question_by <- group_by(subset_df,QuestionID);
summaryTable<- summarize(question_by,
Total_1 = sum(conf.1),Total_2 = sum(conf.2),
Total_3 = sum(conf.3),Total_4=sum(conf.4),
Total_5 = sum(conf.5));
colnames(summaryTable)<-c("Question.ID","conf.1","conf.2","conf.3","conf.4","conf.5");
summaryTable["utility"]<-summaryTable$conf.1+2*summaryTable$conf.2+3*summaryTable$conf.3+
4*summaryTable$conf.4+5*summaryTable$conf.5;
}
confidence_utility<-function(df){
subset_df<-subset(df,select=c(Answer.confidence))
#mark all rows that match the selection
conf.1<- rowSums(subset_df=="1");
subset_df["conf.1"] <- conf.1;
conf.2<- rowSums(subset_df=="2");
subset_df["conf.2"] <- conf.2;
conf.3<- rowSums(subset_df=="3");
subset_df["conf.3"] <- conf.3;
conf.4<- rowSums(subset_df=="4");
subset_df["conf.4"] <- conf.4;
conf.5<- rowSums(subset_df=="5");
subset_df["conf.5"] <- conf.5;
subset_df["QuestionID"] <- df$Question.ID;
subset_df <-subset(subset_df,select= c(QuestionID,conf.1,conf.2,conf.3,conf.4,conf.5));
question_by <- group_by(subset_df,QuestionID);
summaryTable<- summarize(question_by,
Total_1 = sum(conf.1),Total_2 = sum(conf.2),
Total_3 = sum(conf.3),Total_4=sum(conf.4),
Total_5 = sum(conf.5));
colnames(summaryTable)<-c("Question.ID","conf.1","conf.2","conf.3","conf.4","conf.5");
summaryTable["utility"]<-summaryTable$conf.1+2*summaryTable$conf.2+3*summaryTable$conf.3+
4*summaryTable$conf.4+5*summaryTable$conf.5;
return(summaryTable);
}
utility_table<-confidence_utility(df=sampled_dataf);
?sort
utility_table<-sort(utility_table,utility_table$utility,decreasing = TRUE);
utility_table<-data.frame(sort(utility_table,utility_table$utility,decreasing = TRUE));
sort(utility_table,utility_table$utility,decreasing = TRUE)
utility_table<-utility_table[order(utility_table$utility),];
head(utility_table)
utility_table<-utility_table[order(utility_table$utility, decreasing = TRUE),];
head(utility_table)
topQuestions<-utility_table[1:5,];
topQuestions
sampleAnswers<- function(questionList, answers_df){
sampled_dataf<-data.frame();
questionID_f <- data.frame(unique(answers_df$Question.ID));
colnames(questionID_f)<- c("id");
for(id in questionID_f$id){
questionSet<-answers_df[answers_df$Question.ID==id,]
sampled_df<- sample_n(questionSet, 5)
sampled_dataf<-rbind(sampled_dataf,sampled_df);
}
return(sampled_dataf);
}
view(answer_df)
View(answer_df)
View(dataf)
View(dataf)
source('C:/Users/chris/OneDrive/Documentos/GitHub/ML_VotingAggregation/aggregateVotes.R')
outcomes$precision <- 1
outcomes<- data.frame();
outcomes$precision <- 1
outcomes$precision <- 1
colnames(outcomes)<-c("precision","recall","sensibility","sensitivity","accuracy");
colnames(outcomes)<-c("precision","recall","sensibility","sensitivity","accuracy");
outcomes<- data.frame();
colnames(outcomes)<-c("precision","recall","sensibility","sensitivity","accuracy");
outcomes<- data.frame(1:5);
colnames(outcomes)<-c("precision","recall","sensibility","sensitivity","accuracy");
outcomes<- data.frame(c(1:5));
colnames(outcomes)<-c("precision","recall","sensibility","sensitivity","accuracy");
outcomes<- data.frame(c(1:5));
colnames(outcomes)<-c("precision","recall","sensibility","sensitivity","accuracy");
outcomes
matrix(1:5, ncol = 5, nrow = 1)
outcomes <- matrix(1:5, ncol = 5, nrow = 1)
colnames(outcomes)<-c("precision","recall","sensibility","sensitivity","accuracy");
outcomes
outcomes$precision <- 1
outcomes
outcomes<- matrix();
outcomes
colnames(outcomes)<-c("precision","recall","sensibility","sensitivity","accuracy");
colnames(outcomes)<-c("precision");
outcomes
outcomes$precision<-1
outcomes
outcomes<- data.frame(1;2;3;4;5);
outcomes<- list(precision=0, recall=0, sensibility=0, sensitivity=0, accuracy=0);
outcomesMatrix <- rbind(outcomesMatrix,outcomes);
outcomesf<-data.frame(outcomes);
accumOutcomes <- rbind(accumOutcomes,outcomesf);
accumOutcomes <- rbind(accumOutcomes,outcomesf);
accumOutcomes<- data.frame();
accumOutcomes <- rbind(outcomesf,outcomesf);
accumOutcomes<- list();
accumOutcomes <- rbind(accumOutcomes,outcomesf);
accumOutcomes
outcomes<- list(precision=1, recall=0, sensibility=0, sensitivity=0, accuracy=0);
outcomesf<-data.frame(outcomes);
accumOutcomes <- rbind(accumOutcomes,outcomesf);
accumOutcomes
data<- c(2:2.5,0.1)
data
data<- c(2:2.5)
data
?c
c(2:5)
start <- 2
end <- 2.5
data <- seq(a,b,sign(b-a)*0.5)
data <- seq(start,end,sign(b-a)*0.5)
data <- seq(start,end,sign(end-start)*0.5)
data
data <- seq(start,end,sign(end-start)*0.1)
data
logY <- log(data)
logY
x <- c(1:6)
x
plot(x=x,y=data)
plot(x=x,y=logY)
invLogY=1/logY
invData <- 1/data
plot(x=x,y=invData)
plot(x=x,y=invLogY)
plot(x=x,y=invData)
logY <- log2(data)
invLogY=1/logY
plot(x=x,y=invLogY)
install.packages("rJava")
library(caret);
library(car);
# load xml and pmml library
library("devtools")
#install_git("git://github.com/jpmml/r2pmml.git")
library(r2pmml)
library(XML)
# load data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
datasetSize="10K";
linear = paste0("Linear",datasetSize,".csv");
discontinous = paste0("Discontinous",datasetSize,".csv");
saturating = paste0("Saturating",datasetSize,".csv");
all = paste0("ALL",datasetSize,".csv");
datasetName <- c(linear,discontinous,saturating,all);
folder <- "data//New4Cases//";
# Load data section -------------------------------------------------------
library(caret);
library("devtools")
library(r2pmml)
file.info("C:\Users\chris\Documents\R\win-library\3.3\r2pmml\java\jaxb-core-2.3.0")
file.info("C://Users//chris//Documents//R//win-library//3.3//r2pmml//java//jaxb-core-2.3.0")
?jclassPath
?.jclassPath
?.jaddClassPath
library(rJava)
?.jaddClassPath
library(rJava)
library(caret);
library(car);
library(rJava);
library(caret);
library(car);
library(rJava);
# load xml and pmml library
library("devtools")
library(r2pmml)
library(XML)
library(XML)
install.packages("XML")
library(XML)
#Load utility functions
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
#Folder with training data
folder <- "data//1K-3K-9K//";
#
# CONTROL CODE   ------------------------------------------------------------
modelList <- c("Linear","Discontinuous","Saturating","ALL");
datasetSize <- c("1K","3K","9K");
modelName <- modelList[4];
datasetName <- generateDataSetNames(modelName,datasetSize,0);
for(i in c(1:length(datasetName))){
# Generate the dataset names that will be trained -------------------------
generateDataSetNames <- function(modelName,datasetSize,s_idx){
if(s_idx==0 & length(datasetSize)>0){#Generate for all sizes
datasetName <- paste0(modelName,datasetSize[1]);
for(i in c(2:length(datasetSize))){
datasetName <- cbind(datasetName,paste0(modelName,datasetSize[i]));
}
}
else{
datasetName <- paste0(modelName,datasetSize[s_idx]);
}
return(datasetName);
}
# Save results to file ----------------------------------------------------
resultsToFile <- function(mcResults,modelName){
fileName <- paste0("mcResultsf_",modelName,".csv");
write.table(mcResultsf,fileName,sep=",",col.names = TRUE);
mcResultsf
}
# Prepare features --------------------------------------------------------
prepareFeatures <- function(dataf,selectionType){
#Do feature selection (or not)
if(selectionType=="ALL")
featuresdf<- select_ALL(dataf)
else
if(selectionType=="Linear")
featuresdf<- select_Linear(dataf)
else
if(selectionType=="Discontinuous")
featuresdf<- select_Discontinuous(dataf)
else
if(selectionType=="Saturating")
featuresdf<- select_Saturation(dataf)
#Remove zero utilities
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
# Scramble data
featuresdf <- scrambleData(datadf=featuresdf);
return (featuresdf);
}
# Train function  ---------------------------------------------------------
trainModel <- function(featuresdf){
inputFeatures <- dim(featuresdf)[2] - 1; #last column is the target variable
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)# booster="gbtree")
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 2500,
early_stopping_rounds = 500, metrics='rmse',verbose = FALSE)
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
return(list(xgb.model,xgboost.cv));
}
# Validation -------------------------------------------------------------
validatePredictions <- function(modelList, mcResultsf,validationData){
xgb.model <- modelList[[1]];
xgboost.cv <- modelList[[2]];
best_iteration <- xgboost.cv$best_iteration;
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$UTILITY_INCREASE;
best_iteration <- xgboost.cv$best_iteration;
mcResultsf$DataSet[i]<-datasetName[i];
mcResultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
mcResultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
mcResultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
mcResultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
mcResultsf$RMSE[i] <- rmse(error);
mcResultsf$R_Squared[i] <- r_squared(y_pred,validationData$UTILITY_INCREASE);
mcResultsf$MAPD[i] <- mapd(y_pred,validationData$UTILITY_INCREASE);
return(mcResultsf);
}
# Generate PMML file ------------------------------------------------------
generatePMML <- function(xgb.model, featuresdf,modelName){
inputFeatures <- dim(featuresdf)[2] - 1; #last column is the target variable
# Generate feature map
xgboost.fmap = r2pmml::genFMap(featuresdf[1:inputFeatures])
r2pmml::writeFMap(xgboost.fmap, "xgboost.fmap")
# Save the model in XGBoost proprietary binary format
xgb.save(xgb.model, "xgboost.model")
# Dump the model in text format
#  xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap");
pmmlFileName <- paste0(".//pmml///",modelName,"-xgb.pmml");
r2pmml(xgb.model, pmmlFileName, fmap = xgboost.fmap, response_name = "UTILITY_INCREASE",
missing = NULL, ntreelimit = 25, compact = TRUE)
}
library(xgboost)
library(r2pmml) #https://github.com/jpmml/r2pmml
# Initialization section ------------------------------------------------------
#Load utility functions
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
#Folder with training data
folder <- "data//1K-3K-9K//";
#
# CONTROL CODE   ------------------------------------------------------------
modelList <- c("Linear","Discontinuous","Saturating","ALL");
datasetSize <- c("1K","3K","9K");
modelName <- modelList[4];
datasetName <- generateDataSetNames(modelName,datasetSize,0);
for(i in c(1:length(datasetName))){
for(i in c(1:length(datasetName))){
# i <- 3
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
featuresdf <- prepareFeatures(dataf,"ALL");
#Extract training ad validation sets
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
#Train model
outcomeList <- trainModel(trainingData);
#Compute results
mcResultsf <- validatePredictions(outcomeList,mcResultsf,validationData);
}
print(mcResultsf); #show on the console
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
#Folder with training data
folder <- "data//1K-3K-9K//";
#
# CONTROL CODE   ------------------------------------------------------------
modelList <- c("Linear","Discontinuous","Saturating","ALL");
datasetSize <- c("1K","3K","9K");
modelName <- modelList[4];
d
datasetName <- generateDataSetNames(modelName,datasetSize,0);
generateDataSetNames <- function(modelName,datasetSize,s_idx){
if(s_idx==0 & length(datasetSize)>0){#Generate for all sizes
datasetName <- paste0(modelName,datasetSize[1]);
for(i in c(2:length(datasetSize))){
datasetName <- cbind(datasetName,paste0(modelName,datasetSize[i]));
}
}
else{
datasetName <- paste0(modelName,datasetSize[s_idx]);
}
return(datasetName);
}
# Save results to file ----------------------------------------------------
resultsToFile <- function(mcResults,modelName){
fileName <- paste0("mcResultsf_",modelName,".csv");
write.table(mcResultsf,fileName,sep=",",col.names = TRUE);
mcResultsf
}
# Prepare features --------------------------------------------------------
prepareFeatures <- function(dataf,selectionType){
#Do feature selection (or not)
if(selectionType=="ALL")
featuresdf<- select_ALL(dataf)
else
if(selectionType=="Linear")
featuresdf<- select_Linear(dataf)
else
if(selectionType=="Discontinuous")
featuresdf<- select_Discontinuous(dataf)
else
if(selectionType=="Saturating")
featuresdf<- select_Saturation(dataf)
#Remove zero utilities
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
# Scramble data
featuresdf <- scrambleData(datadf=featuresdf);
return (featuresdf);
}
# Train function  ---------------------------------------------------------
trainModel <- function(featuresdf){
inputFeatures <- dim(featuresdf)[2] - 1; #last column is the target variable
xgb.train.data = xgb.DMatrix(data.matrix(trainingData[,1:inputFeatures]),
label = trainingData[,"UTILITY_INCREASE"],
missing = NA)
param <- list(objective = "reg:linear", base_score = 0.5)# booster="gbtree")
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, nfold = 10, nrounds = 2500,
early_stopping_rounds = 500, metrics='rmse',verbose = FALSE)
best_iteration <- xgboost.cv$best_iteration;
xgboost.cv$evaluation_log[best_iteration]
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
return(list(xgb.model,xgboost.cv));
}
# Validation -------------------------------------------------------------
validatePredictions <- function(modelList, mcResultsf,validationData){
xgb.model <- modelList[[1]];
xgboost.cv <- modelList[[2]];
best_iteration <- xgboost.cv$best_iteration;
y_pred <- predict(xgb.model, as.matrix(validationData));
error <- y_pred - validationData$UTILITY_INCREASE;
best_iteration <- xgboost.cv$best_iteration;
mcResultsf$DataSet[i]<-datasetName[i];
mcResultsf$Train_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_mean;
mcResultsf$Train_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$train_rmse_std;
mcResultsf$Test_RMSE_MEAN[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_mean;
mcResultsf$Test_RMSE_STD[i]<-xgboost.cv$evaluation_log[best_iteration]$test_rmse_std;
mcResultsf$RMSE[i] <- rmse(error);
mcResultsf$R_Squared[i] <- r_squared(y_pred,validationData$UTILITY_INCREASE);
mcResultsf$MAPD[i] <- mapd(y_pred,validationData$UTILITY_INCREASE);
return(mcResultsf);
}
# Generate PMML file ------------------------------------------------------
generatePMML <- function(xgb.model, featuresdf,modelName){
inputFeatures <- dim(featuresdf)[2] - 1; #last column is the target variable
# Generate feature map
xgboost.fmap = r2pmml::genFMap(featuresdf[1:inputFeatures])
r2pmml::writeFMap(xgboost.fmap, "xgboost.fmap")
# Save the model in XGBoost proprietary binary format
xgb.save(xgb.model, "xgboost.model")
# Dump the model in text format
#  xgb.dump(xgb.model, "xgboost.model.txt", fmap = "xgboost.fmap");
pmmlFileName <- paste0(".//pmml///",modelName,"-xgb.pmml");
r2pmml(xgb.model, pmmlFileName, fmap = xgboost.fmap, response_name = "UTILITY_INCREASE",
missing = NULL, ntreelimit = 25, compact = TRUE)
}
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
folder <- "data//1K-3K-9K//";
modelList <- c("Linear","Discontinuous","Saturating","ALL");
datasetSize <- c("1K","3K","9K");
modelName <- modelList[4];
datasetName <- generateDataSetNames(modelName,datasetSize,0);
for(i in c(1:length(datasetName))){
# i <- 3
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
featuresdf <- prepareFeatures(dataf,"ALL");
#Extract training ad validation sets
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
#Train model
outcomeList <- trainModel(trainingData);
#Compute results
mcResultsf <- validatePredictions(outcomeList,mcResultsf,validationData);
}
print(mcResultsf); #show on the console
library(xgboost)
library(r2pmml) #https://github.com/jpmml/r2pmml
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
folder <- "data//1K-3K-9K//";
modelList <- c("Linear","Discontinuous","Saturating","ALL");
datasetSize <- c("1K","3K","9K");
modelName <- modelList[4];
datasetName <- generateDataSetNames(modelName,datasetSize,0);
i <- 3
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
featuresdf <- prepareFeatures(dataf,"ALL");
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
outcomeList <- trainModel(trainingData);
for(i in c(1:length(datasetName))){
# i <- 3
fileName <- paste0(folder,datasetName[i],".csv");
dataf <- loadData(fileName);
featuresdf <- prepareFeatures(dataf,"ALL");
#Extract training ad validation sets
totalData = dim(featuresdf)[1];
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
trainingData<- as.data.frame(featuresdf[1:trainingSize,]);
validationData<-as.data.frame(featuresdf[startTestIndex:totalData,]);
#Train model
outcomeList <- trainModel(trainingData);
#Compute results
mcResultsf <- validatePredictions(outcomeList,mcResultsf,validationData);
}
print(mcResultsf); #show on the console
resultsToFile(mcResults,modelName); #save to a .csv file
