#Load utility functions
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
library(xgboost)
library(r2pmml) #https://github.com/jpmml/r2pmml
a <- {1.4, 2.5, 3.1, 5.5}
a <- c(1.4, 2.5, 3.1, 5.5)
b <- c(2.1, 3.9, 4.3, 6.7)
sd(a)
sd(b)
1/2*100
100 * 1/2
100 * 2/3
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
#Download and install the latest H2O package for R.
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))
library(h2o)
h2o.init()
demo(h2o.kmeans)
install.packages("gbm")
install.packages("devtools")
install_git("git://github.com/jpmml/r2pmml.git")
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//DataPoints_1K-3K-9K//";
modelList <- c("Linear","Discontinuous","Saturating","ALL");
modelName <- modelList[1];
datasetSize <- c("1K","3K","9K");
datasetName <- generateDataSetNames(modelName,datasetSize,0);
# Generate the dataset names that will be trained ----------------------------
generateDataSetNames <- function(modelName,datasetSize,s_idx){
if(s_idx==0 & length(datasetSize)>0){#Generate for all sizes
datasetName <- paste0(modelName,datasetSize[1]);
for(i in c(2:length(datasetSize))){
datasetName <- cbind(datasetName,paste0(modelName,datasetSize[i]));
}
}
else{
datasetName <- paste0(modelName,datasetSize[s_idx]);
}
return(datasetName);
}
# Save results to file ----------------------------------------------------
resultsToFile <- function(mcResults,modelName,extension){
fileName <- paste0("mcResultsf_",modelName,extension);
write.table(mcResults,fileName,sep=",",col.names = TRUE);
print(paste0("file written:",fileName));
mcResults
}
# Prepare features --------------------------------------------------------
prepareFeatures <- function(dataf,selectionType){
#Do feature selection (or not)
if(selectionType=="ALL")
featuresdf<- select_ALL(dataf)
else
if(selectionType=="Linear")
featuresdf<- select_Linear(dataf)
else
if(selectionType=="Discontinuous")
featuresdf<- select_Discontinuous(dataf)
else
if(selectionType=="Saturating")
featuresdf<- select_Saturation(dataf)
#Remove zero utilities
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
# Scramble data
featuresdf <- scrambleData(datadf=featuresdf);
return (featuresdf);
}
library(devtools)
library(xgboost)
library(lightgbm, quietly=TRUE)
library(r2pmml) #https://github.com/jpmml/r2pmml
library(lgbm)
devtools::install_github("Laurae2/lgbdl", force = TRUE)
lgb.dl(commit = "master",
compiler = "vs",
repo = "https://github.com/Microsoft/LightGBM")
library(lightgbm)
install_github("Microsoft/LightGBM", subdir = "R-package")
install.packages("devtools")
library(devtools)
install_github("Microsoft/LightGBM", subdir = "R-package")
library(devtools)
options(devtools.install.args = "--no-multiarch") # if you have 64-bit R only, you can skip this
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
library(lightgbm)
data(agaricus.train, package='lightgbm')
train <- agaricus.train
dtrain <- lgb.Dataset(train$data, label=train$label)
params <- list(objective="regression", metric="l2")
model <- lgb.cv(params, dtrain, 10, nfold=5, min_data=1, learning_rate=1, early_stopping_rounds=10)
library(devtools)
library(devtools)
library(xgboost)
library(lightgbm, quietly=TRUE)
library(r2pmml) #https://github.com/jpmml/r2pmml
library(devtools)
options(devtools.install.args = "--no-multiarch") # if you have 64-bit R only, you can skip this
install_github("Microsoft/LightGBM", subdir = "R-package")
# Initialization section ------------------------------------------------------
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//loadData.R");
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//models//xboostRegression.R");
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//models//gbmRegression.R");
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//models//lightGbmRegression.R");
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//DataPoints_1K-3K-9K//";
model.name <- c("Linear","Discontinuous","Saturating","ALL")[1];
method.name <- c("GBM","XGBoost","LigthGBM")[3];
dataset.name.list <- generateDataSetNames(model.name, c("1K","3K","9K"),0);
results.df <- data.frame(matrix(data=NA,nrow=3,ncol=9));
colnames(results.df) <- c("Item","Utility_Type","RMSE","R_Squared", "MAPD","User_Time","Sys_Time","Elapsed_Time","Number_of_Trees");
results_line <- 0;
#for(i in c(1:length(dataset.name.list))){
#for(nodes in c(1:10)){
i <- 1
fileName <- paste0(folder,dataset.name.list[i],".csv");
data.df <- loadData(fileName);
features.df <- prepareFeatures(data.df,"ALL");
#Extract training and validation sets
totalData.size <- dim(features.df)[1];
training.size <- trunc(totalData.size * 0.7);
startTestIndex <- totalData.size - training.size;
training.df <- as.data.frame(features.df[1:training.size,]);
validation.df <-as.data.frame(features.df[startTestIndex:totalData.size,]);
#Train model
numberOfTrees=500;
kfolds=10;
outcome.list <- trainLightGBM(training.df,numberOfTrees,kfolds);
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//loadData.R");
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//models//xboostRegression.R");
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//models//gbmRegression.R");
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//models//lightGbmRegression.R");
#Data structure to keep results
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//DataPoints_1K-3K-9K//";
#folder <- "//DataPoints_1K-3K-9K//";
# CONTROL CODE   ------------------------------------------------------------
model.name <- c("Linear","Discontinuous","Saturating","ALL")[1];
method.name <- c("GBM","XGBoost","LigthGBM")[3];
dataset.name.list <- generateDataSetNames(model.name, c("1K","3K","9K"),0);
results.df <- data.frame(matrix(data=NA,nrow=3,ncol=9));
colnames(results.df) <- c("Item","Utility_Type","RMSE","R_Squared", "MAPD","User_Time","Sys_Time","Elapsed_Time","Number_of_Trees");
results_line <- 0;
#for(i in c(1:length(dataset.name.list))){
#for(nodes in c(1:10)){
i <- 1
# results_line <- results_line+1;
fileName <- paste0(folder,dataset.name.list[i],".csv");
data.df <- loadData(fileName);
features.df <- prepareFeatures(data.df,"ALL");
#Extract training and validation sets
totalData.size <- dim(features.df)[1];
training.size <- trunc(totalData.size * 0.7);
startTestIndex <- totalData.size - training.size;
training.df <- as.data.frame(features.df[1:training.size,]);
validation.df <-as.data.frame(features.df[startTestIndex:totalData.size,]);
#Train model
numberOfTrees=500;
kfolds=10;
outcome.list <- trainLightGBM(training.df,numberOfTrees,kfolds);
?lgb.Dataset
as.matrix(training.df)
lgb.train.data <- lgb.Dataset(as.matrix(training.df), label=training.df$UTILITY_INCREASE);
#system.time gets the time to train the model
time <- system.time(
trained.model <- lgb.cv(params.lgb, lgb.train.data)
);
params.lgb = list(
objective = "regression"
, metric = "l2"
, min_data_in_leaf = 1
, min_sum_hessian_in_leaf = 100
, feature_fraction = 1
, bagging_fraction = 1
, bagging_freq = 0
, early_stopping_round=50
, max_depth=max.depth
, num_leaves = num.leaves
, learning_rate=0.1
, nfold = kfolds
)
max.depth <- 12;
num.leaves <- max.depth^2-1;
params.lgb = list(
objective = "regression"
, metric = "l2"
, min_data_in_leaf = 1
, min_sum_hessian_in_leaf = 100
, feature_fraction = 1
, bagging_fraction = 1
, bagging_freq = 0
, early_stopping_round=50
, max_depth=max.depth
, num_leaves = num.leaves
, learning_rate=0.1
, nfold = kfolds
)
#system.time gets the time to train the model
time <- system.time(
trained.model <- lgb.cv(params.lgb, lgb.train.data)
);
# Get feature importance
lgb.feature.imp = lgb.importance(trained.model, percentage = TRUE)
data("agaricus.train", package = "lightgbm")
train <- agaricus.train
head(train)
head(train$data)
train$label
head(train$label)
?data
matrix.training.df <- matrix(as.numeric(unlist(training.df)),nrow=nrow(training.df));
lgb.train.data <- lgb.Dataset(matrix.training.df, label=matrix.training.df$UTILITY_INCREASE);
lgb.train.data <- lgb.Dataset(matrix.training.df, label=training.df$UTILITY_INCREASE);
time <- system.time(
trained.model <- lgb.cv(params.lgb, lgb.train.data)
);
lgb.cv
?lgb.cv
kfolds <- 10;
params.lgb = list(
objective = "regression"
, metric = "l2"
, min_data_in_leaf = 1
, min_sum_hessian_in_leaf = 100
, feature_fraction = 1
, bagging_fraction = 1
, bagging_freq = 0
, early_stopping_round=50
, max_depth=max.depth
, num_leaves = num.leaves
, learning_rate=0.1
, nfold = kfolds
)
matrix.training.df <- matrix(as.numeric(unlist(training.df)),nrow=nrow(training.df));
lgb.train.data <- lgb.Dataset(matrix.training.df, label=training.df$UTILITY_INCREASE);
#system.time gets the time to train the model
time <- system.time(
trained.model <- lgb.cv(params.lgb, lgb.train.data)
);
max.depth <- 12;
num.leaves <- max.depth^2-1;
kfolds <- 10;
params.lgb = list(
objective = "regression"
, metric = "l2"
, min_data_in_leaf = 1
, min_sum_hessian_in_leaf = 100
, feature_fraction = 1
, bagging_fraction = 1
, bagging_freq = 0
, early_stopping_rounds=50
, max_depth=max.depth
, num_leaves = num.leaves
, learning_rate=0.1
, boosting = dart
)
matrix.training.df <- matrix(as.numeric(unlist(training.df)),nrow=nrow(training.df));
params.lgb = list(
objective = "regression"
, metric = "l2"
, min_data_in_leaf = 1
, min_sum_hessian_in_leaf = 100
, feature_fraction = 1
, bagging_fraction = 1
, bagging_freq = 0
, early_stopping_rounds=50
, max_depth=max.depth
, num_leaves = num.leaves
, learning_rate=0.1
, boosting = "dart"
)
matrix.training.df <- matrix(as.numeric(unlist(training.df)),nrow=nrow(training.df));
lgb.train.data <- lgb.Dataset(matrix.training.df, label=training.df$UTILITY_INCREASE);
#system.time gets the time to train the model
time <- system.time(
trained.model <- lgb.cv(params.lgb, lgb.train.data, nfold = 10, nrounds=10)
);
# Get feature importance
lgb.feature.imp = lgb.importance(trained.model, percentage = TRUE)
summary(trained.model
)
summary(trained.model);
params <- list(objective = "regression", metric = "l2")
model <- lgb.cv(params,
lgb.train.data ,
10,
nfold = 5,
min_data = 1,
learning_rate = 1,
early_stopping_rounds = 10)
model
params.lgb = list(
objective = "regression"
, metric = "l2"
, min_data_in_leaf = 1
, min_sum_hessian_in_leaf = 100
, feature_fraction = 1
, bagging_fraction = 1
, bagging_freq = 0
, early_stopping_rounds=50
, max_depth=max.depth
, num_leaves = num.leaves
, learning_rate=0.1
, boosting = "dart"
)
max.depth <- 12;
num.leaves <- max.depth^2-1;
kfolds <- 10;
params.lgb = list(
objective = "regression"
, metric = "l2"
, min_data_in_leaf = 1
, min_sum_hessian_in_leaf = 100
, feature_fraction = 1
, bagging_fraction = 1
, bagging_freq = 0
, early_stopping_rounds=50
, max_depth=max.depth
, num_leaves = num.leaves
, learning_rate=0.1
, boosting = "dart"
)
matrix.training.df <- matrix(as.numeric(unlist(training.df)),nrow=nrow(training.df));
lgb.train.data <- lgb.Dataset(matrix.training.df, label=training.df$UTILITY_INCREASE);
time <- system.time(
trained.model <- lgb.cv(params.lgb, lgb.train.data, nfold = 10, nrounds=10)
);
time <- system.time(
trained.model <- lgb.cv(params.lgb, lgb.train.data, nfold = 10, nrounds=100)
);
trained.model
